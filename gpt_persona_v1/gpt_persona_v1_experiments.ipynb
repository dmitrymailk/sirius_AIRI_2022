{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import importlib\n",
    "import os\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = { \n",
    "    \"<sp_1>\": \"<sp_1>\",\n",
    "    \"</sp_1>\": \"</sp_1>\",\n",
    "    \"<sp_2>\": \"<sp_2>\",\n",
    "    \"</sp_2>\": \"</sp_2>\",\n",
    "    \"<persona>\": \"<persona>\",\n",
    "    \"</persona>\": \"</persona>\",\n",
    "}\n",
    "config = AutoConfig.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# config.n_positions = 512 \n",
    "# config.n_embd = 1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    def __init__(self, \n",
    "        model=None, \n",
    "        tokenizer=None, \n",
    "        dataloader_train=None,\n",
    "        dataloader_valid=None,\n",
    "        dataloader_test=None,\n",
    "        loss_func_class=None,\n",
    "        estimate_func_class=None,\n",
    "        experiment_config=None,\n",
    "        optimizer_class=None,\n",
    "        sheduler_class=None,\n",
    "        project_name=None,\n",
    "        notebook_name=None,\n",
    "        name_run=\"\",\n",
    "        model_description=\"\",\n",
    "        do_unit_tests=True,\n",
    "        pretrained_model_name=None\n",
    "        ): \n",
    "        assert notebook_name != None, f\"notebook_name should be valid filename, but get {notebook_name}\"\n",
    "\n",
    "        # datasets\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        \n",
    "        # wandb\n",
    "        self.notebook_name = notebook_name\n",
    "        self.project_name = project_name \n",
    "        self.experiment_config = experiment_config\n",
    "        self.wandb_run = None\n",
    "        self.name_run = name_run\n",
    "        self.model_description = model_description\n",
    "        self.model_name = \"pytorch_model\"\n",
    "        self.pure_model_name = \"pytorch_model\"\n",
    "        self.model_artifact = None\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.sheduler_class = sheduler_class\n",
    "        self.loss_func_class = loss_func_class\n",
    "        self.estimate_func_class = estimate_func_class\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = None\n",
    "        self.sheduler = None\n",
    "        self.loss_func = None\n",
    "        self.estimate_func = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.device = torch.device('cpu')\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        # prepare for experiment\n",
    "        self.setup()\n",
    "        if do_unit_tests:\n",
    "            self.unit_tests()\n",
    "\n",
    "    def setup(self):\n",
    "        self.model.to(self.device)\n",
    "        # - Freeze selective layers:\n",
    "        # - Freeze all layers except last n:\n",
    "        if self.experiment_config['freeze_layers'] > 0:\n",
    "            for parameter in self.model.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "            for i, m in enumerate(self.model.transformer.h):        \n",
    "                #Only un-freeze the last n transformer blocks\n",
    "                if i+1 > 12 - self.experiment_config['freeze_layers']:\n",
    "                    for parameter in m.parameters():\n",
    "                        parameter.requires_grad = True \n",
    "\n",
    "            for parameter in self.model.transformer.ln_f.parameters():        \n",
    "                parameter.requires_grad = True\n",
    "\n",
    "            for parameter in self.model.lm_head.parameters():        \n",
    "                parameter.requires_grad = True\n",
    "        if self.experiment_config['do_weight_decay']:\n",
    "            # Prepare optimizer and schedule (linear warmup and decay)\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.experiment_config['weight_decay'],\n",
    "                },\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            self.optimizer = self.optimizer_class(optimizer_grouped_parameters, **self.experiment_config['optimizer'])\n",
    "        else:\n",
    "            self.optimizer = self.optimizer_class(self.model.parameters(), **self.experiment_config['optimizer'])\n",
    "\n",
    "        if self.sheduler_class != None:\n",
    "            # num_training_steps = len(self.dataloader_train) // self.experiment_config[\"sheduler\"] * self.experiment_config['epochs']\n",
    "            self.sheduler = self.sheduler_class(\n",
    "                self.optimizer, \n",
    "                **self.experiment_config['sheduler']\n",
    "                )\n",
    "\n",
    "        self.loss_func = self.loss_func_class()\n",
    "        self.estimate_func = self.estimate_func_class()\n",
    "\n",
    "        # setup wandb\n",
    "        # save model structure and weights to wandb\n",
    "        self.model_artifact = wandb.Artifact(\n",
    "            self.name_run, type=\"model\",\n",
    "            description=self.model_description,\n",
    "            metadata=self.experiment_config)\n",
    "\n",
    "\n",
    "    def get_date(self):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y__%H:%M:%S\")\n",
    "        return date_time\n",
    "\n",
    "    def unit_tests(self):\n",
    "        # test training\n",
    "        X, y = next(iter(self.dataloader_train))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "        loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        # loss = self.loss_func(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # test valid\n",
    "        X, y = next(iter(self.dataloader_valid))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        test_loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "        # test_loss = self.estimate_func(pred, y).item()\n",
    "        # correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # initial validation\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                valid_loss = self.model(X, labels=X).loss\n",
    "                # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                # y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += valid_loss\n",
    "                break\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        # print(\"Initial val = \", test_loss)\n",
    "\n",
    "        print(\"tests ok\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=qrAWbBV1rd4I\n",
    "        # если попытаться создать переменную чтобы не городить тут код возникает ошибка с wandb!\n",
    "        with wandb.init(project=self.project_name, entity=\"dimweb\",\n",
    "                        settings=wandb.Settings(\n",
    "                            # start_method=\"thread\", \n",
    "                            # symlink=False\n",
    "                            ),\n",
    "                        # reinit=True,\n",
    "                        name=self.name_run,\n",
    "                        config=self.experiment_config,\n",
    "                        # sync_tensorboard=True\n",
    "                        ) as run:\n",
    "\n",
    "            self.run = run\n",
    "            \n",
    "            # save model class\n",
    "            # self.save_model_class()\n",
    "\n",
    "            # start train\n",
    "            epochs = self.experiment_config['epochs']\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch: {i}\")\n",
    "                self.train_steps()\n",
    "                self.valid_steps()\n",
    "            \n",
    "            # sync model\n",
    "            self.wandb_save_model()\n",
    "            \n",
    "            print(f\"train end\")\n",
    "    \n",
    "    def save_model_class(self):\n",
    "        # save class\n",
    "        model_class_name = self.experiment_config['model_class_name']\n",
    "        class_script_path_dest = f\"{os.path.join(wandb.run.dir, model_class_name)}.py\"\n",
    "        class_script_path_src = f\"./models/{model_class_name}.py\"\n",
    "        shutil.copy2(class_script_path_src, class_script_path_dest)\n",
    "        self.model_artifact.add_file(class_script_path_dest)\n",
    "        wandb.save(class_script_path_dest)\n",
    "\n",
    "    def wandb_save_model(self):\n",
    "        # wandb использует symlinks для того чтобы сохранять файлы\n",
    "        # но из-за проблем с правами доступа возникает ошибка и модель нельзя сохранить\n",
    "        # поэтому пришлось сохранять модель в дирректорию с самим запуском\n",
    "        # https://docs.wandb.ai/guides/track/advanced/save-restore#example-of-saving-a-file-to-the-wandb-run-directory\n",
    "        # model_save_path = os.path.join(wandb.run.dir, self.model_name)\n",
    "        # optimizer_save_path = os.path.join(wandb.run.dir, f\"optimizer.pt\")\n",
    "        # sheduler_save_path = os.path.join(wandb.run.dir, f\"scheduler.pt\")\n",
    "\n",
    "        saved_path = str(wandb.run.dir).replace(\"/files\", \"_local\")\n",
    "        self.model.save_pretrained(saved_path)\n",
    "        self.tokenizer.save_pretrained(saved_path)\n",
    "        \n",
    "        # torch.save(self.model.state_dict(), model_save_path)\n",
    "        # torch.save(self.optimizer.state_dict(), optimizer_save_path)\n",
    "        # torch.save(self.sheduler.state_dict(), sheduler_save_path)\n",
    "\n",
    "        # self.model_artifact.add_file(model_save_path)\n",
    "        # self.model_artifact.add_file(optimizer_save_path)\n",
    "        # self.model_artifact.add_file(sheduler_save_path)\n",
    "\n",
    "        # wandb.save(model_save_path)\n",
    "        # wandb.save(optimizer_save_path)\n",
    "        # wandb.save(sheduler_save_path)\n",
    "\n",
    "        self.experiment_config['saved_path'] = saved_path \n",
    "\n",
    "        # save notebook\n",
    "        notebook_path = os.path.join(wandb.run.dir, self.notebook_name)\n",
    "        shutil.copy2(self.notebook_name, notebook_path)\n",
    "        self.model_artifact.add_file(notebook_path)\n",
    "        wandb.save(notebook_path)\n",
    "\n",
    "        wandb.log_artifact(self.model_artifact)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        raise NotImplementedError(\"You need specify training steps\")\n",
    "\n",
    "    def valid_steps(self):\n",
    "        raise NotImplementedError(\"You need specify valid steps\")\n",
    "    \n",
    "    def load_model(self, artifact_name=\"\"):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_config = model_artifact.metadata\n",
    "            model_folder = model_config['saved_path'] \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            self.model.to(self.device)\n",
    "            self.free_gpu_cache()\n",
    "    \n",
    "    def free_gpu_cache(self):\n",
    "        print(\"Initial GPU Usage\")\n",
    "        gpu_usage()                             \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        cuda.select_device(0)\n",
    "\n",
    "        print(\"GPU Usage after emptying the cache\")\n",
    "        gpu_usage()\n",
    "\n",
    "    def test(self, artifact_name=\"\"):\n",
    "        raise NotImplementedError(\"You need specify test steps\")\n",
    "\n",
    "\n",
    "class Experiment(BaseExperiment):\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Experiment, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        self.model.train()\n",
    "        interval = self.experiment_config['check_interval']\n",
    "        \n",
    "        for batch, (X, y) in enumerate(self.dataloader_train):\n",
    "            # Send data to training device\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            loss = self.model(X, labels=X).loss\n",
    "            perplexity = torch.exp(torch.tensor(loss.item()))\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.sheduler != None:\n",
    "                self.sheduler.step()\n",
    "            \n",
    "            # Progress output\n",
    "            if batch % interval == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"train_perplexity\": perplexity})\n",
    "\n",
    "    def valid_steps(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                loss = self.model(X, labels=y).loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        perplexity = torch.exp(torch.tensor(test_loss))\n",
    "        \n",
    "        wandb.log({\"val_loss\": test_loss})\n",
    "        wandb.log({\"valid_perplexity\": perplexity})\n",
    "    \n",
    "    @staticmethod\n",
    "    def last_index(array, elem):\n",
    "        return len(array) - 1 - array[::-1].index(elem)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(artifact_name=\"\", persona=\"\", user_inputs=None, interact=False, cuda=False):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_folder = \"\"\n",
    "            if ':' in artifact_name:\n",
    "                model_artifact = wandb.use_artifact(artifact_name)\n",
    "                model_dir = model_artifact.download()\n",
    "                model_config = model_artifact.metadata\n",
    "                model_folder = model_config['saved_path'] \n",
    "            else:\n",
    "                model_folder = artifact_name\n",
    "            print(model_folder)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
    "            model = model.to(device)\n",
    "            print(\"Start conversation\")\n",
    "            print(f\"Persona: {persona}\")\n",
    "            persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}{SPECIAL_TOKENS['</persona>']}\"\n",
    "            # persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}\"\n",
    "            persona_ids = tokenizer.encode(persona, return_tensors='pt')\n",
    "            persona_ids = persona_ids.to(device)\n",
    "            VOCAB_TOKENS = tokenizer.get_added_vocab()\n",
    "\n",
    "            last_index = 0\n",
    "            steps = len(user_inputs)\n",
    "            history = []\n",
    "            \n",
    "            if interact:\n",
    "                steps = 15\n",
    "                file = open('conversation.txt', 'w')\n",
    "            # global_step\n",
    "            for step in range(steps):\n",
    "                # print(\"-\"*40,step)\n",
    "                if interact:\n",
    "                    user_input = input()\n",
    "                    file.write(f\"User: {user_input}\\n\")\n",
    "                else:\n",
    "                    user_input = user_inputs[step]\n",
    "                print(f\"User: {user_input}\")\n",
    "\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</sp_1>']} {SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_input}{SPECIAL_TOKENS['</sp_1>']}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                history.append(user_input)\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_inputs[step]}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                new_user_input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "                new_user_input_ids = new_user_input_ids.to(device)\n",
    "                # chat_history_ids = chat_history_ids.to(device) \n",
    "                history_chat = \"\".join(history[-3:])\n",
    "                # print(\"-\"*100)\n",
    "                # print(history_chat)\n",
    "                # print(\"-\"*100)\n",
    "                history_ids = tokenizer.encode(history_chat, return_tensors='pt')\n",
    "                history_ids = history_ids.to(device)\n",
    "                # chat_history_ids = \n",
    "                bot_input_ids = torch.cat([persona_ids, history_ids], dim=-1)\n",
    "                \n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(bot_input_ids[0]))\n",
    "                # print(\"-\"*100)\n",
    "\n",
    "                # generated a response while limiting the total chat history to 1000 tokens, \n",
    "                model_response = model.generate(\n",
    "                    bot_input_ids, \n",
    "                    max_length=350,\n",
    "                    pad_token_id=tokenizer.eos_token_id,  \n",
    "                    # no_repeat_ngram_size=3,       \n",
    "                    do_sample=True, \n",
    "                    # num_beams=3, \n",
    "                    # early_stopping=True\n",
    "                    # top_k=100,\n",
    "                    # top_k=100,\n",
    "                    # top_p=0.7,\n",
    "                    temperature = 0.95,\n",
    "                    top_k=100, \n",
    "                    top_p=0.95,\n",
    "                )\n",
    "\n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(model_response[0]))\n",
    "                # print(\"-\"*100)\n",
    "                model_response = model_response.to(device)\n",
    "                model_response_list = list(model_response[0])\n",
    "                # end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS['</sp_2>'])\n",
    "                # last_index = end_speaker_index\n",
    "                end_speaker_index = Experiment.last_index(model_response_list, VOCAB_TOKENS['</sp_2>'])\n",
    "                model_response = model_response[:, :end_speaker_index+1]\n",
    "\n",
    "                chat_history_ids = model_response\n",
    "                bot_response_decode = tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True) \n",
    "                last_history = history[-1]\n",
    "                last_history = f\"{last_history}{bot_response_decode}{SPECIAL_TOKENS['</sp_2>']}\"\n",
    "                \n",
    "                # print(\"-\"*100)\n",
    "                history[-1] = last_history\n",
    "                # print(history)\n",
    "                # print(\"-\"*100)\n",
    "                print(f\"Bot: {bot_response_decode}\")\n",
    "                if interact:\n",
    "                    file.write(f\"Bot: {bot_response_decode}\\n\")\n",
    "                # print(history)\n",
    "            \n",
    "            if interact:\n",
    "                file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n",
    "# persona_chat_original = persona_chat_original\n",
    "# persona_chat_original = persona_chat_original[:3000]\n",
    "# persona_chat_original.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatGenerator:\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.processed_dataset = []\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.process_dataset()\n",
    "\n",
    "\tdef process_dataset(self):\n",
    "\t\tprocessed_dataset = {\n",
    "\t\t\t\"persona\": [],\n",
    "\t\t\t\"history\": [],\n",
    "\t\t\t# \"target\": []\n",
    "\t\t}\n",
    "\n",
    "\t\tsp_1_start = SPECIAL_TOKENS['<sp_1>']\n",
    "\t\tsp_1_end = SPECIAL_TOKENS['</sp_1>']\n",
    "\t\tsp_2_start = SPECIAL_TOKENS['<sp_2>']\n",
    "\t\tsp_2_end = SPECIAL_TOKENS['</sp_2>']\n",
    "\t\tpersona_start = SPECIAL_TOKENS['<persona>']\n",
    "\t\tpersona_end = SPECIAL_TOKENS['</persona>']\n",
    "\t\trelu = lambda x: x if x > 0 else 0 \n",
    "\t\tfor i in range(len(self.initial_dataset)):\n",
    "\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n",
    "\t\t\t# persona = f\"{persona_start} {persona} {persona_end}\"\n",
    "\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n",
    "\t\t\tchat = chat[:-1]\n",
    "\t\t\thistory = []\n",
    "\t\t\tfor j in range(len(chat)):\n",
    "\t\t\t\treply = chat[j]\n",
    "\t\t\t\tif (j+1) % 2 == 0:\n",
    "\t\t\t\t\t# reply = f\"{sp_2_start} {reply} {sp_2_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_2_start}{reply}{sp_2_end}\"\n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\t\t\t\t\t# temp_his = f\"{history}{tokenizer.eos_token}\"\n",
    "\t\t\t\t\ttemp_history = history[relu(j-4):j+1]\n",
    "\t\t\t\t\ttemp_history = \"\".join(temp_history)\n",
    "\t\t\t\t\t# temp_history = \"\".join(history)\n",
    "\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n",
    "\t\t\t\t\tprocessed_dataset['history'].append(temp_history)\n",
    "\t\t\t\t\t# processed_dataset['target'].append(reply)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# reply = f\"{sp_1_start} {reply} {sp_1_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_1_start}{reply}{sp_1_end}\"\n",
    "\t\t\t\t\t# history += reply \n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\n",
    "\t\tdataset = pd.DataFrame(data=processed_dataset)\n",
    "\t\treturn dataset\n",
    "\n",
    "train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.01)\n",
    "train_dataset_csv, valid_dataset_csv = train_dataset_csv.reset_index(), valid_dataset_csv.reset_index()\n",
    "train_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=train_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=valid_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_generator.process_dataset().to_csv(\"./process_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatDataset(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None,\n",
    "\t\tis_validation=False\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.is_validation = is_validation\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.initial_dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.initial_dataset.iloc[idx]\n",
    "\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n",
    "\t\tif not self.is_validation:\n",
    "\t\t\trandom.shuffle(persona)\n",
    "\t\tpersona = [item+\". \" for item in persona]\n",
    "\t\tpersona[-1] = persona[-1][:-1]\n",
    "\t\tpersona = [SPECIAL_TOKENS['<persona>']] + persona + [SPECIAL_TOKENS['</persona>']]\n",
    "\t\t# persona = [SPECIAL_TOKENS['<persona>']] + persona\n",
    "\t\t# print(persona)\n",
    "\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n",
    "\t\tpersona = torch.cat([*persona])\n",
    "\n",
    "\t\thistory = row['history']\n",
    "\t\t# print(history)\n",
    "\t\thistory = self.tokenizer.encode(history)\n",
    "\t\thistory = torch.tensor(history).flatten()\n",
    "\t\n",
    "\t\tfeature = torch.cat([persona, history])\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"feature\": feature,\n",
    "\t\t\t\"target\": feature \n",
    "\t\t}\n",
    "\n",
    "train_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=train_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=valid_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer,\n",
    "\tis_validation=True\n",
    ")\n",
    "\n",
    "def collate(examples):\n",
    "\tfeatures = [item['feature'] for item in examples]\n",
    "\tfeatures = pad_sequence(features, batch_first=True)\n",
    "\t\n",
    "\ttarget = [item['target'] for item in examples]\n",
    "\ttarget = pad_sequence(features, batch_first=True)\n",
    "\treturn features.to(torch.long), target.to(torch.long) \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "\tbatch_size=4, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last = True,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "\tbatch_size=8, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last=False,\n",
    "\tshuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<persona>i am a jack of all, master of none. i am male. i am unsociable. i live to not disappoint my loved ones. my weakness are sweets.</persona><sp_2>i am a sort of jack of all trades, i do odd jobs here and there.</sp_2><sp_1>oh that is so cool! you must be very handy!</sp_1><sp_2>yup! i help people do all sorts of stuff, i love helping out family and all that.</sp_2><sp_1>me too. i am devout catholic and help community when i can!</sp_1><sp_2>that is amazing! what do you do in your spare time?</sp_2>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[4]['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем dumb модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "tests ok\n"
     ]
    }
   ],
   "source": [
    "exp_config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"check_interval\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 5e-4\n",
    "    },\n",
    "    \"model_name\": \"pytorch_model\",\n",
    "    \"saved_path\": \"\",\n",
    "    \"do_weight_decay\": False,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"freeze_layers\": 1\n",
    "}\n",
    "\n",
    "exp_config[\"sheduler\"] = {\n",
    "    # \"max_lr\": 0.01, \n",
    "    # \"steps_per_epoch\": len(train_dataloader), \n",
    "    # \"epochs\": exp_config[\"epochs\"]\n",
    "    # \"step_size\": 25\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"num_training_steps\": len(train_dataloader)\n",
    "\n",
    "}\n",
    "model =  AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\", config=config, ignore_mismatched_sizes=True)\n",
    "# model =  AutoModelForCausalLM.from_pretrained(\"/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_230130-aurwatvq_local\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = GPT2LMModel(**exp_config['model_args'])\n",
    "\n",
    "# не хочу создавать глобальные переменные \n",
    "exp_params = {\n",
    "    \"model\": model, \n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"dataloader_train\": train_dataloader,\n",
    "    \"dataloader_valid\": valid_dataloader,\n",
    "    \"dataloader_test\": valid_dataloader,\n",
    "    \"loss_func_class\": nn.CrossEntropyLoss,\n",
    "    \"estimate_func_class\": nn.CrossEntropyLoss,\n",
    "    \"experiment_config\": exp_config,\n",
    "    \"optimizer_class\": torch.optim.AdamW,\n",
    "    \"sheduler_class\": None,\n",
    "    \"notebook_name\": \"gpt_persona_v1.ipynb\",\n",
    "    \"project_name\": \"gpt_persona_bot\",\n",
    "    \"name_run\": \"persona_gpt\",\n",
    "    \"model_description\": \"добавил линейный scheduler\",\n",
    "    \"do_unit_tests\": True,\n",
    "}\n",
    "\n",
    "experiment_test = Experiment(**exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16266"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220711_171905-u49i12fh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/u49i12fh\" target=\"_blank\">persona_gpt</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "experiment_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are you up to this evening ?\n",
      "air in my area is fresh that is why i play sports\n",
      "did you just come in from playing sports ?\n",
      "i am working out now , and drinking water\n",
      "i just got done sewing a new shirt\n",
      "no , i am working out now , and watching tv\n",
      "i have no time for tv being an art teacher\n",
      "that is nice of you , i am getting a degree at school now\n",
      "oh really what are you getting your degree in\n",
      "it is good to be busy sometimes , keeps you focus\n",
      "it really does which is why i love to sew my own clothing\n",
      "i am working on psychology because i had 2 car crashes in past\n",
      "oh wow do you like to visit thrift shops ?\n",
      "it affects my driving now , because of fear .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset_csv.iloc[0]['chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220711_213808-221s2omb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/221s2omb\" target=\"_blank\">restful-sea-234</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_230742-8ryub57u_local\n",
      "Start conversation\n",
      "Persona:  i like to hike. i have a golden retriever. i write fiction for fun. i work in advertising. my mother is dead.\n",
      "User: hi how are you my girl just left me\n",
      "Bot: hi i am okay i do advertising for a livingthat is so cool. i am a pharmacist at a local storethat sounds nice i work in the ad field for big pharma\n",
      "User: its ok she left because i do not like pizza\n",
      "Bot: why? i love to hike i like to write fiction.\n",
      "User: i just like to eat fish while i read\n",
      "Bot: what does your favorite fiction have in it\n",
      "User: i like that too i do not even have a tv because i am never inside\n",
      "Bot: what do you do for fun\n",
      "User: that sounds like fun i am a teacher\n",
      "Bot: what are you teaching there?\n",
      "User: art i like it because it is easy\n",
      "Bot: what languages do you speak\n",
      "User: that is true i enjoy reading a lot\n",
      "Bot: i also like to travel and write fiction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc8d614a0f5477d895249a11b4c0bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">restful-sea-234</strong>: <a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/221s2omb\" target=\"_blank\">https://wandb.ai/dimweb/gpt_persona_bot/runs/221s2omb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220711_213808-221s2omb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# persona = train_dataset_csv['Persona'][1]\n",
    "number_example = 6\n",
    "persona = valid_dataset_csv['Persona'][number_example]\n",
    "# persona = \"i like catsi like to travelfavorite color is greeni got a new jobi like cars\"\n",
    "user_inputs = [\n",
    "    \"Hi. What is your name?\",\n",
    "    \"What do you like?\",\n",
    "    \"What is your job?\",\n",
    "\t\"Tell me about yourself please.\"\n",
    "\t# \"Where is your mom?\",\n",
    "\t# \"Fuck you leather man!\"\n",
    "]\n",
    "\n",
    "user_inputs = [item for i, item in enumerate(valid_dataset_csv.iloc[number_example]['chat'].split(\"\\n\")) if len(item) and (i+1) % 2 != 0]\n",
    "\n",
    "Experiment.test(\n",
    "\tartifact_name=\"/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_230742-8ryub57u_local\",\n",
    "\t# artifact_name=\"/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220711_171905-u49i12fh_local\",\n",
    "\tpersona=persona,\n",
    "\tuser_inputs=user_inputs,\n",
    "\tinteract=False,\n",
    "\tcuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "persona = \"\"\"\n",
    "The bravest thing I've ever done was give a product demonstration to a live audience of over one thousand people.\n",
    "My role model is my dad because he immigrated to Canada when he was only 18, got a job right away, and worked hard to provide for our family.\n",
    "I have worked in marketing for over half my life.\n",
    "I have travelled to over 30 countries for business. My favourite one was Japan because they have a lot of delicious food.\n",
    "The first time I ever moved away from home was participating in a job internship 500 miles away from my hometown.\n",
    "I'm currently reading a book about financial technology to better familiarize myself with the industry and its ongoing trends.\n",
    "I can speak four different languages: English, Spanish, Italian, and French.\n",
    "I was 17 when I got my first job at a fast-food restaurant, which helped me develop leadership, communication and listening skills.\n",
    "My favourite subject in school was English, which is why I decided to become a writer.\n",
    "I once set the record for most products sold in one day during my previous sales job.\n",
    "My previous supervisor would describe me as reliable since I regularly submitted high-quality work by their respective deadlines.\n",
    "One of my hidden talents is negotiating with others, which I believe is what makes me a strong sales representative.\n",
    "My favourite part about my job is pitching unique advertising campaign ideas that help small businesses stand out from competitors.\n",
    "One of my main professional goals is to one day become the president of a financial corporation./em>\n",
    "Something I can improve about myself is my software knowledge, which is why I currently take technology courses during the evenings and weekends.\n",
    "My favourite hobby is hiking on the Bruce Trail every weekend.\n",
    "My dream vacation is going to Cappadocia and riding in a hot air balloon.\n",
    "I love to cook. My signature dish is homemade spaghetti and meatballs.\n",
    "I'm currently taking Spanish courses because I hope to visit Spain at the end of the year.\n",
    "If I could have any superpower, it would be to speak any language so I could connect with anyone in the world.\n",
    "An accomplishment I'm most proud of is teaching myself how to play the guitar in three months.\n",
    "If I could eat one type of cuisine for the rest of my life, it would be Italian food, as I enjoy pasta and pizza.\n",
    "When I was younger, I wanted to be a pilot, which led me to later earn my pilot's license.\n",
    "I have a collection of mugs. I get a new mug in every country I go to, so I currently have 24.\n",
    "In high school and college, I starred as a lead in four different musicals\n",
    "I have five siblings. Two older brothers and two younger sisters. We're all close in age, so most of us grew up attending the same schools as each other.\n",
    "I had two pet hamsters when I was younger.\n",
    "My favourite meal to cook is salted egg, chicken and rice. I serve it at holiday gatherings with my family every year.\n",
    "The first car I ever owned was a red sedan.\n",
    "My favourite animal is a dog because they're so energetic and friendly.\n",
    "\"\"\"\n",
    "\n",
    "class SentenceRanker:\n",
    "\tdef __init__(self, persona_sentences=None, sentence_model=None):\n",
    "\t\tself.persona_sentences = persona_sentences\n",
    "\t\tself.sentence_model = sentence_model\n",
    "\t\tself.sentence_embeddings = self.sentence_model.encode(\n",
    "\t\t\tpersona_sentences, \n",
    "\t\t\tconvert_to_tensor=True\n",
    "\t\t)\n",
    "\t\tself.ranked_sentences = {}\n",
    "\t\n",
    "\tdef rank_sentences(self, query, k=5):\n",
    "\t\tkey = f\"{query}_{k}\"\n",
    "\t\tif self.ranked_sentences.get(key, False):\n",
    "\t\t\treturn self.ranked_sentences[key]\n",
    "\t\tuser_sentence_embeddings = sentence_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "\t\tcos_sim_ranks = self.cos_sim(\n",
    "\t\t\tuser_sentence_embeddings,\n",
    "\t\t\tself.sentence_embeddings\n",
    "\t\t)\n",
    "\n",
    "\t\ttop_indices = torch.argsort(cos_sim_ranks, descending=True)\n",
    "\t\ttop_indices = list(top_indices[:k].cpu().numpy())\n",
    "\t\tsimilar_sentences = [self.persona_sentences[idx] for idx in top_indices]\n",
    "\t\tself.ranked_sentences[key] = similar_sentences \n",
    "\t\treturn similar_sentences\n",
    "\t\n",
    "\tdef cos_sim(self, a, b):\n",
    "\t\ta_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "\t\tb_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "\t\treturn torch.sum(a_norm * b_norm, dim=1)\n",
    "\n",
    "\n",
    "persona_sentences = persona.split(\"\\n\")\n",
    "persona_sentences = [item for item in persona_sentences if len(item) > 0]\n",
    "sentence_ranker = SentenceRanker(\n",
    "\tpersona_sentences=persona_sentences,\n",
    "\tsentence_model=sentence_model\n",
    ")\n",
    "user_sentence = [\n",
    "\t\"Do you have a dream?\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['Do you have a dream?']_4\": ['My dream vacation is going to Cappadocia and riding in a hot air balloon.',\n",
       "  'My role model is my dad because he immigrated to Canada when he was only 18, got a job right away, and worked hard to provide for our family.',\n",
       "  \"When I was younger, I wanted to be a pilot, which led me to later earn my pilot's license.\",\n",
       "  'If I could have any superpower, it would be to speak any language so I could connect with anyone in the world.'],\n",
       " \"['Do you have a dream?']_5\": ['My dream vacation is going to Cappadocia and riding in a hot air balloon.',\n",
       "  'My role model is my dad because he immigrated to Canada when he was only 18, got a job right away, and worked hard to provide for our family.',\n",
       "  \"When I was younger, I wanted to be a pilot, which led me to later earn my pilot's license.\",\n",
       "  'If I could have any superpower, it would be to speak any language so I could connect with anyone in the world.',\n",
       "  'One of my main professional goals is to one day become the president of a financial corporation./em>']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ranker.ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My dream vacation is going to Cappadocia and riding in a hot air balloon.',\n",
       " 'My role model is my dad because he immigrated to Canada when he was only 18, got a job right away, and worked hard to provide for our family.',\n",
       " \"When I was younger, I wanted to be a pilot, which led me to later earn my pilot's license.\",\n",
       " 'If I could have any superpower, it would be to speak any language so I could connect with anyone in the world.',\n",
       " 'One of my main professional goals is to one day become the president of a financial corporation./em>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ranker.rank_sentences(\n",
    "\tuser_sentence, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
