{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import importlib\n",
    "import os\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = { \n",
    "    \"<sp_1>\": \"<sp_1>\",\n",
    "    \"</sp_1>\": \"</sp_1>\",\n",
    "    \"<sp_2>\": \"<sp_2>\",\n",
    "    \"</sp_2>\": \"</sp_2>\",\n",
    "    \"<persona>\": \"<persona>\",\n",
    "    \"</persona>\": \"</persona>\",\n",
    "}\n",
    "config = AutoConfig.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# config.n_positions = 512 \n",
    "# config.n_embd = 1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    def __init__(self, \n",
    "        model=None, \n",
    "        tokenizer=None, \n",
    "        dataloader_train=None,\n",
    "        dataloader_valid=None,\n",
    "        dataloader_test=None,\n",
    "        loss_func_class=None,\n",
    "        estimate_func_class=None,\n",
    "        experiment_config=None,\n",
    "        optimizer_class=None,\n",
    "        sheduler_class=None,\n",
    "        project_name=None,\n",
    "        notebook_name=None,\n",
    "        name_run=\"\",\n",
    "        model_description=\"\",\n",
    "        do_unit_tests=True,\n",
    "        pretrained_model_name=None\n",
    "        ): \n",
    "        assert notebook_name != None, f\"notebook_name should be valid filename, but get {notebook_name}\"\n",
    "\n",
    "        # datasets\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        \n",
    "        # wandb\n",
    "        self.notebook_name = notebook_name\n",
    "        self.project_name = project_name \n",
    "        self.experiment_config = experiment_config\n",
    "        self.wandb_run = None\n",
    "        self.name_run = name_run\n",
    "        self.model_description = model_description\n",
    "        self.model_name = \"pytorch_model\"\n",
    "        self.pure_model_name = \"pytorch_model\"\n",
    "        self.model_artifact = None\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.sheduler_class = sheduler_class\n",
    "        self.loss_func_class = loss_func_class\n",
    "        self.estimate_func_class = estimate_func_class\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = None\n",
    "        self.sheduler = None\n",
    "        self.loss_func = None\n",
    "        self.estimate_func = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.device = torch.device('cpu')\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        # prepare for experiment\n",
    "        self.setup()\n",
    "        if do_unit_tests:\n",
    "            self.unit_tests()\n",
    "\n",
    "    def setup(self):\n",
    "        self.model.to(self.device)\n",
    "        # self.model.transformer.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.experiment_config['optimizer'])\n",
    "        \n",
    "        if self.sheduler_class != None:\n",
    "            self.sheduler = self.sheduler_class(self.optimizer, **self.experiment_config['sheduler'])\n",
    "\n",
    "        self.loss_func = self.loss_func_class()\n",
    "        self.estimate_func = self.estimate_func_class()\n",
    "\n",
    "        # setup wandb\n",
    "        # save model structure and weights to wandb\n",
    "        self.model_artifact = wandb.Artifact(\n",
    "            self.name_run, type=\"model\",\n",
    "            description=self.model_description,\n",
    "            metadata=self.experiment_config)\n",
    "\n",
    "\n",
    "    def get_date(self):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y__%H:%M:%S\")\n",
    "        return date_time\n",
    "\n",
    "    def unit_tests(self):\n",
    "        # test training\n",
    "        X, y = next(iter(self.dataloader_train))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "        loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        # loss = self.loss_func(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # test valid\n",
    "        X, y = next(iter(self.dataloader_valid))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        test_loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "        # test_loss = self.estimate_func(pred, y).item()\n",
    "        # correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # initial validation\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                valid_loss = self.model(X, labels=X).loss\n",
    "                # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                # y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += valid_loss\n",
    "                break\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        # print(\"Initial val = \", test_loss)\n",
    "\n",
    "        print(\"tests ok\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=qrAWbBV1rd4I\n",
    "        # если попытаться создать переменную чтобы не городить тут код возникает ошибка с wandb!\n",
    "        with wandb.init(project=self.project_name, entity=\"dimweb\",\n",
    "                        settings=wandb.Settings(\n",
    "                            start_method=\"thread\", \n",
    "                            # symlink=False\n",
    "                            ),\n",
    "                        reinit=True,\n",
    "                        name=self.name_run,\n",
    "                        config=self.experiment_config,\n",
    "                        # sync_tensorboard=True\n",
    "                        ) as run:\n",
    "\n",
    "            self.run = run\n",
    "            \n",
    "            # save model class\n",
    "            # self.save_model_class()\n",
    "\n",
    "            # start train\n",
    "            epochs = self.experiment_config['epochs']\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch: {i}\")\n",
    "                self.train_steps()\n",
    "                self.valid_steps()\n",
    "            \n",
    "            # sync model\n",
    "            self.wandb_save_model()\n",
    "            \n",
    "            print(f\"train end\")\n",
    "    \n",
    "    def save_model_class(self):\n",
    "        # save class\n",
    "        model_class_name = self.experiment_config['model_class_name']\n",
    "        class_script_path_dest = f\"{os.path.join(wandb.run.dir, model_class_name)}.py\"\n",
    "        class_script_path_src = f\"./models/{model_class_name}.py\"\n",
    "        shutil.copy2(class_script_path_src, class_script_path_dest)\n",
    "        self.model_artifact.add_file(class_script_path_dest)\n",
    "        wandb.save(class_script_path_dest)\n",
    "\n",
    "    def wandb_save_model(self):\n",
    "        # wandb использует symlinks для того чтобы сохранять файлы\n",
    "        # но из-за проблем с правами доступа возникает ошибка и модель нельзя сохранить\n",
    "        # поэтому пришлось сохранять модель в дирректорию с самим запуском\n",
    "        # https://docs.wandb.ai/guides/track/advanced/save-restore#example-of-saving-a-file-to-the-wandb-run-directory\n",
    "        # model_save_path = os.path.join(wandb.run.dir, self.model_name)\n",
    "        # optimizer_save_path = os.path.join(wandb.run.dir, f\"optimizer.pt\")\n",
    "        # sheduler_save_path = os.path.join(wandb.run.dir, f\"scheduler.pt\")\n",
    "\n",
    "        saved_path = str(wandb.run.dir).replace(\"/files\", \"_local\")\n",
    "        self.model.save_pretrained(saved_path)\n",
    "        self.tokenizer.save_pretrained(saved_path)\n",
    "        \n",
    "        # torch.save(self.model.state_dict(), model_save_path)\n",
    "        # torch.save(self.optimizer.state_dict(), optimizer_save_path)\n",
    "        # torch.save(self.sheduler.state_dict(), sheduler_save_path)\n",
    "\n",
    "        # self.model_artifact.add_file(model_save_path)\n",
    "        # self.model_artifact.add_file(optimizer_save_path)\n",
    "        # self.model_artifact.add_file(sheduler_save_path)\n",
    "\n",
    "        # wandb.save(model_save_path)\n",
    "        # wandb.save(optimizer_save_path)\n",
    "        # wandb.save(sheduler_save_path)\n",
    "\n",
    "        self.experiment_config['saved_path'] = saved_path \n",
    "\n",
    "        # save notebook\n",
    "        notebook_path = os.path.join(wandb.run.dir, self.notebook_name)\n",
    "        shutil.copy2(self.notebook_name, notebook_path)\n",
    "        self.model_artifact.add_file(notebook_path)\n",
    "        wandb.save(notebook_path)\n",
    "\n",
    "        wandb.log_artifact(self.model_artifact)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        raise NotImplementedError(\"You need specify training steps\")\n",
    "\n",
    "    def valid_steps(self):\n",
    "        raise NotImplementedError(\"You need specify valid steps\")\n",
    "    \n",
    "    def load_model(self, artifact_name=\"\"):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_config = model_artifact.metadata\n",
    "            model_folder = model_config['saved_path'] \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            self.model.to(self.device)\n",
    "            self.free_gpu_cache()\n",
    "    \n",
    "    def free_gpu_cache(self):\n",
    "        print(\"Initial GPU Usage\")\n",
    "        gpu_usage()                             \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        cuda.select_device(0)\n",
    "\n",
    "        print(\"GPU Usage after emptying the cache\")\n",
    "        gpu_usage()\n",
    "\n",
    "    def test(self, artifact_name=\"\"):\n",
    "        raise NotImplementedError(\"You need specify test steps\")\n",
    "\n",
    "\n",
    "class Experiment(BaseExperiment):\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Experiment, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        self.model.train()\n",
    "        interval = self.experiment_config['check_interval']\n",
    "        \n",
    "        for batch, (X, y) in enumerate(self.dataloader_train):\n",
    "            # Send data to training device\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            loss = self.model(X, labels=X).loss\n",
    "            perplexity = torch.exp(torch.tensor(loss.item()))\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.sheduler != None:\n",
    "                self.sheduler.step()\n",
    "            \n",
    "            # Progress output\n",
    "            if batch % interval == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"train_perplexity\": perplexity})\n",
    "\n",
    "    def valid_steps(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                loss = self.model(X, labels=y).loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        perplexity = torch.exp(torch.tensor(test_loss))\n",
    "        \n",
    "        wandb.log({\"val_loss\": test_loss})\n",
    "        wandb.log({\"valid_perplexity\": perplexity})\n",
    "    \n",
    "    @staticmethod\n",
    "    def test(artifact_name=\"\", persona=\"\", user_inputs=None, interact=False, cuda=False):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            \n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_folder = model_config['saved_path'] \n",
    "            print(model_folder)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
    "            model = model.to(device)\n",
    "            print(\"Start conversation\")\n",
    "            print(persona)\n",
    "            persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}{SPECIAL_TOKENS['</persona>']}\"\n",
    "            # persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}\"\n",
    "            persona_ids = tokenizer.encode(persona, return_tensors='pt')\n",
    "            persona_ids = persona_ids.to(device)\n",
    "            VOCAB_TOKENS = tokenizer.get_added_vocab()\n",
    "\n",
    "            last_index = 0\n",
    "            steps = len(user_inputs)\n",
    "            history = []\n",
    "            if interact:\n",
    "                steps = 15\n",
    "            # global_step\n",
    "            for step in range(steps):\n",
    "                user_input = user_inputs[step]\n",
    "                if interact:\n",
    "                    user_input = input()\n",
    "                print(f\"User: {user_input}\")\n",
    "\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</sp_1>']} {SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_input}{SPECIAL_TOKENS['</sp_1>']}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                history.append(user_input)\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_inputs[step]}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                new_user_input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "                new_user_input_ids = new_user_input_ids.to(device)\n",
    "                # chat_history_ids = chat_history_ids.to(device) \n",
    "                history_chat = \"\".join(history[step-2:])\n",
    "                print(\"-\"*100)\n",
    "                print(history_chat)\n",
    "                print(\"-\"*100)\n",
    "                history_ids = tokenizer.encode(history_chat, return_tensors='pt')\n",
    "                history_ids = history_ids.to(device)\n",
    "                # chat_history_ids = \n",
    "                bot_input_ids = torch.cat([persona_ids, new_user_input_ids], dim=-1)\n",
    "                \n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(bot_input_ids[0]))\n",
    "                # print(\"-\"*100)\n",
    "\n",
    "                # generated a response while limiting the total chat history to 1000 tokens, \n",
    "                model_response = model.generate(\n",
    "                    bot_input_ids, \n",
    "                    max_length=250,\n",
    "                    pad_token_id=tokenizer.eos_token_id,  \n",
    "                    no_repeat_ngram_size=3,       \n",
    "                    do_sample=True, \n",
    "                    top_k=100,\n",
    "                    top_p=0.7,\n",
    "                    temperature = 0.8,\n",
    "                )\n",
    "\n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(model_response[0]))\n",
    "                # print(\"-\"*100)\n",
    "                model_response = model_response.to(device)\n",
    "                model_response_list = list(model_response[0])\n",
    "                end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS['</sp_2>']) +1\n",
    "                last_index = end_speaker_index+1\n",
    "                model_response = model_response[:, :end_speaker_index+1]\n",
    "\n",
    "                chat_history_ids = model_response\n",
    "                bot_response_decode = tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True) \n",
    "                last_history = history[-1]\n",
    "                last_history = f\"{last_history}{bot_response_decode}{SPECIAL_TOKENS['</sp_2>']}\"\n",
    "                history[-1] = last_history\n",
    "                print(f\"Bot: {bot_response_decode}\")\n",
    "                print(\"-\"*100)\n",
    "                print(history)\n",
    "                print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n",
    "# persona_chat_original = persona_chat_original\n",
    "# persona_chat_original = persona_chat_original[:3000]\n",
    "# persona_chat_original.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatGenerator:\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.processed_dataset = []\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.process_dataset()\n",
    "\n",
    "\tdef process_dataset(self):\n",
    "\t\tprocessed_dataset = {\n",
    "\t\t\t\"persona\": [],\n",
    "\t\t\t\"history\": [],\n",
    "\t\t\t# \"target\": []\n",
    "\t\t}\n",
    "\n",
    "\t\tsp_1_start = SPECIAL_TOKENS['<sp_1>']\n",
    "\t\tsp_1_end = SPECIAL_TOKENS['</sp_1>']\n",
    "\t\tsp_2_start = SPECIAL_TOKENS['<sp_2>']\n",
    "\t\tsp_2_end = SPECIAL_TOKENS['</sp_2>']\n",
    "\t\tpersona_start = SPECIAL_TOKENS['<persona>']\n",
    "\t\tpersona_end = SPECIAL_TOKENS['</persona>']\n",
    "\t\trelu = lambda x: x if x > 0 else 0 \n",
    "\t\tfor i in range(len(self.initial_dataset)):\n",
    "\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n",
    "\t\t\t# persona = f\"{persona_start} {persona} {persona_end}\"\n",
    "\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n",
    "\t\t\tchat = chat[:-1]\n",
    "\t\t\thistory = []\n",
    "\t\t\tfor j in range(len(chat)):\n",
    "\t\t\t\treply = chat[j]\n",
    "\t\t\t\tif (j+1) % 2 == 0:\n",
    "\t\t\t\t\t# reply = f\"{sp_2_start} {reply} {sp_2_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_2_start}{reply}{sp_2_end}\"\n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\t\t\t\t\t# temp_his = f\"{history}{tokenizer.eos_token}\"\n",
    "\t\t\t\t\ttemp_history = history[relu(j-4):j+1]\n",
    "\t\t\t\t\ttemp_history = \"\".join(temp_history)\n",
    "\t\t\t\t\t# temp_history = \"\".join(history)\n",
    "\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n",
    "\t\t\t\t\tprocessed_dataset['history'].append(temp_history)\n",
    "\t\t\t\t\t# processed_dataset['target'].append(reply)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# reply = f\"{sp_1_start} {reply} {sp_1_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_1_start}{reply}{sp_1_end}\"\n",
    "\t\t\t\t\t# history += reply \n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\n",
    "\t\tdataset = pd.DataFrame(data=processed_dataset)\n",
    "\t\treturn dataset\n",
    "\n",
    "train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.01)\n",
    "train_dataset_csv, valid_dataset_csv = train_dataset_csv.reset_index(), valid_dataset_csv.reset_index()\n",
    "train_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=train_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=valid_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatDataset(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None,\n",
    "\t\tis_validation=False\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.is_validation = is_validation\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.initial_dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.initial_dataset.iloc[idx]\n",
    "\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n",
    "\t\tif not self.is_validation:\n",
    "\t\t\trandom.shuffle(persona)\n",
    "\t\tpersona = [item+\". \" for item in persona]\n",
    "\t\tpersona[-1] = persona[-1][:-1]\n",
    "\t\tpersona = [SPECIAL_TOKENS['<persona>']] + persona + [SPECIAL_TOKENS['</persona>']]\n",
    "\t\t# persona = [SPECIAL_TOKENS['<persona>']] + persona\n",
    "\t\t# print(persona)\n",
    "\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n",
    "\t\tpersona = torch.cat([*persona])\n",
    "\n",
    "\t\thistory = row['history']\n",
    "\t\t# print(history)\n",
    "\t\thistory = self.tokenizer.encode(history)\n",
    "\t\thistory = torch.tensor(history).flatten()\n",
    "\t\n",
    "\t\tfeature = torch.cat([persona, history])\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"feature\": feature,\n",
    "\t\t\t\"target\": feature \n",
    "\t\t}\n",
    "\n",
    "train_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=train_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=valid_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer,\n",
    "\tis_validation=True\n",
    ")\n",
    "\n",
    "def collate(examples):\n",
    "\tfeatures = [item['feature'] for item in examples]\n",
    "\tfeatures = pad_sequence(features, batch_first=True)\n",
    "\t\n",
    "\ttarget = [item['target'] for item in examples]\n",
    "\ttarget = pad_sequence(features, batch_first=True)\n",
    "\treturn features.to(torch.long), target.to(torch.long) \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "\tbatch_size=4, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last = True,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "\tbatch_size=8, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last=False,\n",
    "\tshuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<persona>i am the youngest sibling of four. chocolate is my favorite food. i geocache in my spare time. i am learning how to play the piano.</persona><sp_2>i love chocolate, my sisters hate it.</sp_2><sp_1>i stay away from the sun also. as a fair skinned caucasian i burn.</sp_1><sp_2>do you play the piano? i am learning.</sp_2><sp_1>i have tried but i cannot keep my interest in it. do you like it?</sp_1><sp_2>so far, i do. i am not a music person, but my sisters play.</sp_2>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[4]['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем dumb модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "tests ok\n"
     ]
    }
   ],
   "source": [
    "exp_config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"check_interval\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-3\n",
    "    },\n",
    "    \"model_name\": \"pytorch_model\",\n",
    "    \"saved_path\": \"\"\n",
    "}\n",
    "\n",
    "exp_config[\"sheduler\"] = {\n",
    "    \"max_lr\": 0.01, \n",
    "    \"steps_per_epoch\": len(train_dataloader), \n",
    "    \"epochs\": exp_config[\"epochs\"]\n",
    "}\n",
    "model =  AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\", config=config, ignore_mismatched_sizes=True)\n",
    "# model =  AutoModelForCausalLM.from_pretrained(\"/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_230130-aurwatvq_local\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = GPT2LMModel(**exp_config['model_args'])\n",
    "\n",
    "# не хочу создавать глобальные переменные \n",
    "exp_params = {\n",
    "    \"model\": model, \n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"dataloader_train\": train_dataloader,\n",
    "    \"dataloader_valid\": valid_dataloader,\n",
    "    \"dataloader_test\": valid_dataloader,\n",
    "    \"loss_func_class\": nn.CrossEntropyLoss,\n",
    "    \"estimate_func_class\": nn.CrossEntropyLoss,\n",
    "    \"experiment_config\": exp_config,\n",
    "    \"optimizer_class\": torch.optim.Adam,\n",
    "    \"sheduler_class\": None,\n",
    "    \"notebook_name\": \"gpt_persona_v1.ipynb\",\n",
    "    \"project_name\": \"gpt_persona_bot\",\n",
    "    \"name_run\": \"persona_gpt\",\n",
    "    \"model_description\": \"тренирую на полном датасете\",\n",
    "    \"do_unit_tests\": True,\n",
    "}\n",
    "\n",
    "experiment_test = Experiment(**exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_210259-1hqjbplv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/1hqjbplv\" target=\"_blank\">persona_gpt</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "experiment_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_131349-1hb6w8zn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/1hb6w8zn\" target=\"_blank\">solar-frog-141</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220709_022541-cnmyn0dz_local\n",
      "Start conversation\n",
      " i have six siblings. the future scares me. i was adopted when i was a baby. my adopted dad works at hp.\n",
      "User: Hi. What is your name?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<sp_1>Hi. What is your name?</sp_1><sp_2>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: i am not sure if i have a good name, what is it?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: What do you like?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<sp_1>What do you like?</sp_1><sp_2>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2>', '<sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: What is your job?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2><sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2><sp_1>What is your job?</sp_1><sp_2>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2>', '<sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2>', '<sp_1>What is your job?</sp_1><sp_2>i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.</sp_2>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: Where is your mom?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2><sp_1>What is your job?</sp_1><sp_2>i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.</sp_2><sp_1>Where is your mom?</sp_1><sp_2>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: i am not sure if i have a good mommyi am so sorry my dad says so.i know a mommy! i have 6 siblings who are adopted!my dad is a preacher. i do not know if my mommy is ok.ok, then i have to go see my dad. he is a mom.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2>', '<sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2>', '<sp_1>What is your job?</sp_1><sp_2>i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.</sp_2>', '<sp_1>Where is your mom?</sp_1><sp_2>i am not sure if i have a good mommyi am so sorry my dad says so.i know a mommy! i have 6 siblings who are adopted!my dad is a preacher. i do not know if my mommy is ok.ok, then i have to go see my dad. he is a mom.</sp_2>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: Fuck you leather man!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<sp_1>What is your job?</sp_1><sp_2>i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.</sp_2><sp_1>Where is your mom?</sp_1><sp_2>i am not sure if i have a good mommyi am so sorry my dad says so.i know a mommy! i have 6 siblings who are adopted!my dad is a preacher. i do not know if my mommy is ok.ok, then i have to go see my dad. he is a mom.</sp_2><sp_1>Fuck you leather man!</sp_1><sp_2>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: nah kidding, i have been adopted all my life?please tell me i do not have many parents, that is why i only eat fishsounds like me, i am an adultwell, that s okay. i like to fish.my dad works for the church, he is a preacherokay... i am a dad i am the oldest of 5 kidsthe future scares my first two brothers\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<sp_1>Hi. What is your name?</sp_1><sp_2>i am not sure if i have a good name, what is it?</sp_2>', '<sp_1>What do you like?</sp_1><sp_2>i do not really like my parentsi am so sorry. i am a waiter at a waiteri work at a restaurant as a waiter</sp_2>', '<sp_1>What is your job?</sp_1><sp_2>i have a job at hp, i have 6 brothers.nice! i am a stay at home mom so i have a lot of free time!i was adopted, but my dad works for hp.</sp_2>', '<sp_1>Where is your mom?</sp_1><sp_2>i am not sure if i have a good mommyi am so sorry my dad says so.i know a mommy! i have 6 siblings who are adopted!my dad is a preacher. i do not know if my mommy is ok.ok, then i have to go see my dad. he is a mom.</sp_2>', '<sp_1>Fuck you leather man!</sp_1><sp_2>nah kidding, i have been adopted all my life?please tell me i do not have many parents, that is why i only eat fishsounds like me, i am an adultwell, that s okay. i like to fish.my dad works for the church, he is a preacherokay... i am a dad i am the oldest of 5 kidsthe future scares my first two brothers</sp_2>']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce471768476f4ebea388c6215a39fee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-frog-141</strong>: <a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/1hb6w8zn\" target=\"_blank\">https://wandb.ai/dimweb/gpt_persona_bot/runs/1hb6w8zn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220710_131349-1hb6w8zn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# persona = train_dataset_csv['Persona'][1]\n",
    "persona = valid_dataset_csv['Persona'][18]\n",
    "# persona = \"i like catsi like to travelfavorite color is greeni got a new jobi like cars\"\n",
    "user_inputs = [\n",
    "    \"Hi. What is your name?\",\n",
    "    \"What do you like?\",\n",
    "    \"What is your job?\",\n",
    "\t\"Where is your mom?\",\n",
    "\t\"Fuck you leather man!\"\n",
    "]\n",
    "\n",
    "Experiment.test(\n",
    "\tartifact_name=\"persona_gpt:v9\",\n",
    "\tpersona=persona,\n",
    "\tuser_inputs=user_inputs,\n",
    "\tinteract=False,\n",
    "\tcuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 99% |\n",
      "GPU Usage after emptying the cache\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% |  1% |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
