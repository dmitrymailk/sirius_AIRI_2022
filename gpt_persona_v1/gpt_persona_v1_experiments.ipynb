{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import importlib\n",
    "import os\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = { \n",
    "    \"<sp_1>\": \"<sp_1>\",\n",
    "    \"</sp_1>\": \"</sp_1>\",\n",
    "    \"<sp_2>\": \"<sp_2>\",\n",
    "    \"</sp_2>\": \"</sp_2>\",\n",
    "    \"<persona>\": \"<persona>\",\n",
    "    \"</persona>\": \"</persona>\",\n",
    "}\n",
    "config = AutoConfig.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# config.n_positions = 512 \n",
    "# config.n_embd = 1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    def __init__(self, \n",
    "        model=None, \n",
    "        tokenizer=None, \n",
    "        dataloader_train=None,\n",
    "        dataloader_valid=None,\n",
    "        dataloader_test=None,\n",
    "        loss_func_class=None,\n",
    "        estimate_func_class=None,\n",
    "        experiment_config=None,\n",
    "        optimizer_class=None,\n",
    "        sheduler_class=None,\n",
    "        project_name=None,\n",
    "        notebook_name=None,\n",
    "        name_run=\"\",\n",
    "        model_description=\"\",\n",
    "        do_unit_tests=True,\n",
    "        pretrained_model_name=None\n",
    "        ): \n",
    "        assert notebook_name != None, f\"notebook_name should be valid filename, but get {notebook_name}\"\n",
    "\n",
    "        # datasets\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        \n",
    "        # wandb\n",
    "        self.notebook_name = notebook_name\n",
    "        self.project_name = project_name \n",
    "        self.experiment_config = experiment_config\n",
    "        self.wandb_run = None\n",
    "        self.name_run = name_run\n",
    "        self.model_description = model_description\n",
    "        self.model_name = \"pytorch_model\"\n",
    "        self.pure_model_name = \"pytorch_model\"\n",
    "        self.model_artifact = None\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.sheduler_class = sheduler_class\n",
    "        self.loss_func_class = loss_func_class\n",
    "        self.estimate_func_class = estimate_func_class\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = None\n",
    "        self.sheduler = None\n",
    "        self.loss_func = None\n",
    "        self.estimate_func = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.device = torch.device('cpu')\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        # prepare for experiment\n",
    "        self.setup()\n",
    "        if do_unit_tests:\n",
    "            self.unit_tests()\n",
    "\n",
    "    def setup(self):\n",
    "        self.model.to(self.device)\n",
    "        # - Freeze selective layers:\n",
    "        # - Freeze all layers except last n:\n",
    "        if self.experiment_config['freeze_layers'] > 0:\n",
    "            for parameter in self.model.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "            for i, m in enumerate(self.model.transformer.h):        \n",
    "                #Only un-freeze the last n transformer blocks\n",
    "                if i+1 > 12 - self.experiment_config['freeze_layers']:\n",
    "                    for parameter in m.parameters():\n",
    "                        parameter.requires_grad = True \n",
    "\n",
    "            for parameter in self.model.transformer.ln_f.parameters():        \n",
    "                parameter.requires_grad = True\n",
    "\n",
    "            for parameter in self.model.lm_head.parameters():        \n",
    "                parameter.requires_grad = True\n",
    "        if self.experiment_config['do_weight_decay']:\n",
    "            # Prepare optimizer and schedule (linear warmup and decay)\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.experiment_config['weight_decay'],\n",
    "                },\n",
    "                {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            self.optimizer = self.optimizer_class(optimizer_grouped_parameters, **self.experiment_config['optimizer'])\n",
    "        else:\n",
    "            self.optimizer = self.optimizer_class(self.model.parameters(), **self.experiment_config['optimizer'])\n",
    "\n",
    "        if self.sheduler_class != None:\n",
    "            # num_training_steps = len(self.dataloader_train) // self.experiment_config[\"sheduler\"] * self.experiment_config['epochs']\n",
    "            self.sheduler = self.sheduler_class(\n",
    "                self.optimizer, \n",
    "                **self.experiment_config['sheduler']\n",
    "                )\n",
    "\n",
    "        self.loss_func = self.loss_func_class()\n",
    "        self.estimate_func = self.estimate_func_class()\n",
    "\n",
    "        # setup wandb\n",
    "        # save model structure and weights to wandb\n",
    "        self.model_artifact = wandb.Artifact(\n",
    "            self.name_run, type=\"model\",\n",
    "            description=self.model_description,\n",
    "            metadata=self.experiment_config)\n",
    "\n",
    "\n",
    "    def get_date(self):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y__%H:%M:%S\")\n",
    "        return date_time\n",
    "\n",
    "    def unit_tests(self):\n",
    "        # test training\n",
    "        X, y = next(iter(self.dataloader_train))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "        loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        # loss = self.loss_func(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # test valid\n",
    "        X, y = next(iter(self.dataloader_valid))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        test_loss = self.model(X, labels=X).loss\n",
    "        # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        # y = y[..., 1:].contiguous().view(-1)\n",
    "        # test_loss = self.estimate_func(pred, y).item()\n",
    "        # correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # initial validation\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                valid_loss = self.model(X, labels=X).loss\n",
    "                # pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                # y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += valid_loss\n",
    "                break\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        # print(\"Initial val = \", test_loss)\n",
    "\n",
    "        print(\"tests ok\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=qrAWbBV1rd4I\n",
    "        # если попытаться создать переменную чтобы не городить тут код возникает ошибка с wandb!\n",
    "        with wandb.init(project=self.project_name, entity=\"dimweb\",\n",
    "                        settings=wandb.Settings(\n",
    "                            # start_method=\"thread\", \n",
    "                            # symlink=False\n",
    "                            ),\n",
    "                        # reinit=True,\n",
    "                        name=self.name_run,\n",
    "                        config=self.experiment_config,\n",
    "                        # sync_tensorboard=True\n",
    "                        ) as run:\n",
    "\n",
    "            self.run = run\n",
    "            \n",
    "            # save model class\n",
    "            # self.save_model_class()\n",
    "\n",
    "            # start train\n",
    "            epochs = self.experiment_config['epochs']\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch: {i}\")\n",
    "                self.train_steps()\n",
    "                self.valid_steps()\n",
    "            \n",
    "            # sync model\n",
    "            self.wandb_save_model()\n",
    "            \n",
    "            print(f\"train end\")\n",
    "    \n",
    "    def save_model_class(self):\n",
    "        # save class\n",
    "        model_class_name = self.experiment_config['model_class_name']\n",
    "        class_script_path_dest = f\"{os.path.join(wandb.run.dir, model_class_name)}.py\"\n",
    "        class_script_path_src = f\"./models/{model_class_name}.py\"\n",
    "        shutil.copy2(class_script_path_src, class_script_path_dest)\n",
    "        self.model_artifact.add_file(class_script_path_dest)\n",
    "        wandb.save(class_script_path_dest)\n",
    "\n",
    "    def wandb_save_model(self):\n",
    "        # wandb использует symlinks для того чтобы сохранять файлы\n",
    "        # но из-за проблем с правами доступа возникает ошибка и модель нельзя сохранить\n",
    "        # поэтому пришлось сохранять модель в дирректорию с самим запуском\n",
    "        # https://docs.wandb.ai/guides/track/advanced/save-restore#example-of-saving-a-file-to-the-wandb-run-directory\n",
    "        # model_save_path = os.path.join(wandb.run.dir, self.model_name)\n",
    "        # optimizer_save_path = os.path.join(wandb.run.dir, f\"optimizer.pt\")\n",
    "        # sheduler_save_path = os.path.join(wandb.run.dir, f\"scheduler.pt\")\n",
    "\n",
    "        saved_path = str(wandb.run.dir).replace(\"/files\", \"_local\")\n",
    "        self.model.save_pretrained(saved_path)\n",
    "        self.tokenizer.save_pretrained(saved_path)\n",
    "        \n",
    "        # torch.save(self.model.state_dict(), model_save_path)\n",
    "        # torch.save(self.optimizer.state_dict(), optimizer_save_path)\n",
    "        # torch.save(self.sheduler.state_dict(), sheduler_save_path)\n",
    "\n",
    "        # self.model_artifact.add_file(model_save_path)\n",
    "        # self.model_artifact.add_file(optimizer_save_path)\n",
    "        # self.model_artifact.add_file(sheduler_save_path)\n",
    "\n",
    "        # wandb.save(model_save_path)\n",
    "        # wandb.save(optimizer_save_path)\n",
    "        # wandb.save(sheduler_save_path)\n",
    "\n",
    "        self.experiment_config['saved_path'] = saved_path \n",
    "\n",
    "        # save notebook\n",
    "        notebook_path = os.path.join(wandb.run.dir, self.notebook_name)\n",
    "        shutil.copy2(self.notebook_name, notebook_path)\n",
    "        self.model_artifact.add_file(notebook_path)\n",
    "        wandb.save(notebook_path)\n",
    "\n",
    "        wandb.log_artifact(self.model_artifact)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        raise NotImplementedError(\"You need specify training steps\")\n",
    "\n",
    "    def valid_steps(self):\n",
    "        raise NotImplementedError(\"You need specify valid steps\")\n",
    "    \n",
    "    def load_model(self, artifact_name=\"\"):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_config = model_artifact.metadata\n",
    "            model_folder = model_config['saved_path'] \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            self.model.to(self.device)\n",
    "            self.free_gpu_cache()\n",
    "    \n",
    "    def free_gpu_cache(self):\n",
    "        print(\"Initial GPU Usage\")\n",
    "        gpu_usage()                             \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        cuda.select_device(0)\n",
    "\n",
    "        print(\"GPU Usage after emptying the cache\")\n",
    "        gpu_usage()\n",
    "\n",
    "    def test(self, artifact_name=\"\"):\n",
    "        raise NotImplementedError(\"You need specify test steps\")\n",
    "\n",
    "\n",
    "class Experiment(BaseExperiment):\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Experiment, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        self.model.train()\n",
    "        interval = self.experiment_config['check_interval']\n",
    "        \n",
    "        for batch, (X, y) in enumerate(self.dataloader_train):\n",
    "            # Send data to training device\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            loss = self.model(X, labels=X).loss\n",
    "            perplexity = torch.exp(torch.tensor(loss.item()))\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.sheduler != None:\n",
    "                self.sheduler.step()\n",
    "            \n",
    "            # Progress output\n",
    "            if batch % interval == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"train_perplexity\": perplexity})\n",
    "\n",
    "    def valid_steps(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                loss = self.model(X, labels=y).loss\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        perplexity = torch.exp(torch.tensor(test_loss))\n",
    "        \n",
    "        wandb.log({\"val_loss\": test_loss})\n",
    "        wandb.log({\"valid_perplexity\": perplexity})\n",
    "    \n",
    "    @staticmethod\n",
    "    def test(artifact_name=\"\", persona=\"\", user_inputs=None, interact=False, cuda=False):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_folder = \"\"\n",
    "            if ':' in artifact_name:\n",
    "                model_artifact = wandb.use_artifact(artifact_name)\n",
    "                model_dir = model_artifact.download()\n",
    "                model_config = model_artifact.metadata\n",
    "                model_folder = model_config['saved_path'] \n",
    "            else:\n",
    "                model_folder = artifact_name\n",
    "            print(model_folder)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_folder)\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
    "            model = model.to(device)\n",
    "            print(\"Start conversation\")\n",
    "            print(f\"Persona: {persona}\")\n",
    "            persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}{SPECIAL_TOKENS['</persona>']}\"\n",
    "            # persona = f\"{SPECIAL_TOKENS['<persona>']}{persona}\"\n",
    "            persona_ids = tokenizer.encode(persona, return_tensors='pt')\n",
    "            persona_ids = persona_ids.to(device)\n",
    "            VOCAB_TOKENS = tokenizer.get_added_vocab()\n",
    "\n",
    "            last_index = 0\n",
    "            steps = len(user_inputs)\n",
    "            history = []\n",
    "            \n",
    "            if interact:\n",
    "                steps = 15\n",
    "                file = open('conversation.txt', 'w')\n",
    "            # global_step\n",
    "            for step in range(steps):\n",
    "                if interact:\n",
    "                    user_input = input()\n",
    "                    file.write(f\"User: {user_input}\\n\")\n",
    "                else:\n",
    "                    user_input = user_inputs[step]\n",
    "                print(f\"User: {user_input}\")\n",
    "\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</sp_1>']} {SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_input}{SPECIAL_TOKENS['</sp_1>']}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                history.append(user_input)\n",
    "                # user_input = f\"{SPECIAL_TOKENS['<sp_1>']}{user_inputs[step]}{SPECIAL_TOKENS['<sp_2>']}\"\n",
    "                new_user_input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "                new_user_input_ids = new_user_input_ids.to(device)\n",
    "                # chat_history_ids = chat_history_ids.to(device) \n",
    "                history_chat = \"\".join(history[-3:])\n",
    "                # print(\"-\"*100)\n",
    "                # print(history_chat)\n",
    "                # print(\"-\"*100)\n",
    "                history_ids = tokenizer.encode(history_chat, return_tensors='pt')\n",
    "                history_ids = history_ids.to(device)\n",
    "                # chat_history_ids = \n",
    "                bot_input_ids = torch.cat([persona_ids, history_ids], dim=-1)\n",
    "                \n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(bot_input_ids[0]))\n",
    "                # print(\"-\"*100)\n",
    "\n",
    "                # generated a response while limiting the total chat history to 1000 tokens, \n",
    "                model_response = model.generate(\n",
    "                    bot_input_ids, \n",
    "                    max_length=250,\n",
    "                    pad_token_id=tokenizer.eos_token_id,  \n",
    "                    no_repeat_ngram_size=3,       \n",
    "                    do_sample=True, \n",
    "                    top_k=100,\n",
    "                    top_p=0.7,\n",
    "                    temperature = 0.8,\n",
    "                )\n",
    "\n",
    "                # print(\"-\"*100)\n",
    "                # print(tokenizer.decode(model_response[0]))\n",
    "                # print(\"-\"*100)\n",
    "                model_response = model_response.to(device)\n",
    "                model_response_list = list(model_response[0])\n",
    "                end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS['</sp_2>']) +1\n",
    "                last_index = end_speaker_index+1\n",
    "                model_response = model_response[:, :end_speaker_index+1]\n",
    "\n",
    "                chat_history_ids = model_response\n",
    "                bot_response_decode = tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True) \n",
    "                last_history = history[-1]\n",
    "                last_history = f\"{last_history}{bot_response_decode}{SPECIAL_TOKENS['</sp_2>']}\"\n",
    "                \n",
    "                # print(\"-\"*100)\n",
    "                history[-1] = last_history\n",
    "                # print(history)\n",
    "                # print(\"-\"*100)\n",
    "                print(f\"Bot: {bot_response_decode}\")\n",
    "                if interact:\n",
    "                    file.write(f\"Bot: {bot_response_decode}\\n\")\n",
    "                # print(history)\n",
    "            \n",
    "            if interact:\n",
    "                file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n",
    "# persona_chat_original = persona_chat_original\n",
    "# persona_chat_original = persona_chat_original[:3000]\n",
    "# persona_chat_original.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatGenerator:\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.processed_dataset = []\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.process_dataset()\n",
    "\n",
    "\tdef process_dataset(self):\n",
    "\t\tprocessed_dataset = {\n",
    "\t\t\t\"persona\": [],\n",
    "\t\t\t\"history\": [],\n",
    "\t\t\t# \"target\": []\n",
    "\t\t}\n",
    "\n",
    "\t\tsp_1_start = SPECIAL_TOKENS['<sp_1>']\n",
    "\t\tsp_1_end = SPECIAL_TOKENS['</sp_1>']\n",
    "\t\tsp_2_start = SPECIAL_TOKENS['<sp_2>']\n",
    "\t\tsp_2_end = SPECIAL_TOKENS['</sp_2>']\n",
    "\t\tpersona_start = SPECIAL_TOKENS['<persona>']\n",
    "\t\tpersona_end = SPECIAL_TOKENS['</persona>']\n",
    "\t\trelu = lambda x: x if x > 0 else 0 \n",
    "\t\tfor i in range(len(self.initial_dataset)):\n",
    "\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n",
    "\t\t\t# persona = f\"{persona_start} {persona} {persona_end}\"\n",
    "\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n",
    "\t\t\tchat = chat[:-1]\n",
    "\t\t\thistory = []\n",
    "\t\t\tfor j in range(len(chat)):\n",
    "\t\t\t\treply = chat[j]\n",
    "\t\t\t\tif (j+1) % 2 == 0:\n",
    "\t\t\t\t\t# reply = f\"{sp_2_start} {reply} {sp_2_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_2_start}{reply}{sp_2_end}\"\n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\t\t\t\t\t# temp_his = f\"{history}{tokenizer.eos_token}\"\n",
    "\t\t\t\t\ttemp_history = history[relu(j-4):j+1]\n",
    "\t\t\t\t\ttemp_history = \"\".join(temp_history)\n",
    "\t\t\t\t\t# temp_history = \"\".join(history)\n",
    "\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n",
    "\t\t\t\t\tprocessed_dataset['history'].append(temp_history)\n",
    "\t\t\t\t\t# processed_dataset['target'].append(reply)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# reply = f\"{sp_1_start} {reply} {sp_1_end}\"\n",
    "\t\t\t\t\treply = f\"{sp_1_start}{reply}{sp_1_end}\"\n",
    "\t\t\t\t\t# history += reply \n",
    "\t\t\t\t\thistory.append(reply)\n",
    "\n",
    "\t\tdataset = pd.DataFrame(data=processed_dataset)\n",
    "\t\treturn dataset\n",
    "\n",
    "train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.01)\n",
    "train_dataset_csv, valid_dataset_csv = train_dataset_csv.reset_index(), valid_dataset_csv.reset_index()\n",
    "train_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=train_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=valid_dataset_csv,\n",
    "\ttokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatDataset(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None,\n",
    "\t\tis_validation=False\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.is_validation = is_validation\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.initial_dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.initial_dataset.iloc[idx]\n",
    "\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n",
    "\t\tif not self.is_validation:\n",
    "\t\t\trandom.shuffle(persona)\n",
    "\t\tpersona = [item+\". \" for item in persona]\n",
    "\t\tpersona[-1] = persona[-1][:-1]\n",
    "\t\tpersona = [SPECIAL_TOKENS['<persona>']] + persona + [SPECIAL_TOKENS['</persona>']]\n",
    "\t\t# persona = [SPECIAL_TOKENS['<persona>']] + persona\n",
    "\t\t# print(persona)\n",
    "\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n",
    "\t\tpersona = torch.cat([*persona])\n",
    "\n",
    "\t\thistory = row['history']\n",
    "\t\t# print(history)\n",
    "\t\thistory = self.tokenizer.encode(history)\n",
    "\t\thistory = torch.tensor(history).flatten()\n",
    "\t\n",
    "\t\tfeature = torch.cat([persona, history])\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"feature\": feature,\n",
    "\t\t\t\"target\": feature \n",
    "\t\t}\n",
    "\n",
    "train_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=train_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=valid_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer,\n",
    "\tis_validation=True\n",
    ")\n",
    "\n",
    "def collate(examples):\n",
    "\tfeatures = [item['feature'] for item in examples]\n",
    "\tfeatures = pad_sequence(features, batch_first=True)\n",
    "\t\n",
    "\ttarget = [item['target'] for item in examples]\n",
    "\ttarget = pad_sequence(features, batch_first=True)\n",
    "\treturn features.to(torch.long), target.to(torch.long) \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "\tbatch_size=4, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last = True,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "\tbatch_size=8, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last=False,\n",
    "\tshuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<persona>i now live in new mexico. i am an orphan. i grew up in nevada. i like dogs.</persona><sp_2>i lost my parents as well.</sp_2><sp_1>i did turn to food immediately after, hamburgers mainly. then, i got a dog.</sp_1><sp_2>i have two dogs and they are my best friends.</sp_2><sp_1>what kind do you have? mine is a lab. he is such a goof</sp_1><sp_2>a lab mix and an irish setter.</sp_2>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[4]['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем dumb модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized because the shapes did not match:\n",
      "- transformer.wte.weight: found shape torch.Size([50257, 1024]) in the checkpoint and torch.Size([50263, 1024]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([50257, 1024]) in the checkpoint and torch.Size([50263, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "tests ok\n"
     ]
    }
   ],
   "source": [
    "exp_config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"check_interval\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-3\n",
    "    },\n",
    "    \"model_name\": \"pytorch_model\",\n",
    "    \"saved_path\": \"\",\n",
    "    \"do_weight_decay\": False,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"freeze_layers\": 3\n",
    "}\n",
    "\n",
    "exp_config[\"sheduler\"] = {\n",
    "    # \"max_lr\": 0.01, \n",
    "    # \"steps_per_epoch\": len(train_dataloader), \n",
    "    # \"epochs\": exp_config[\"epochs\"]\n",
    "    # \"step_size\": 25\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"num_training_steps\": len(train_dataloader)\n",
    "\n",
    "}\n",
    "model =  AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\", config=config, ignore_mismatched_sizes=True)\n",
    "# model =  AutoModelForCausalLM.from_pretrained(\"/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_230130-aurwatvq_local\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = GPT2LMModel(**exp_config['model_args'])\n",
    "\n",
    "# не хочу создавать глобальные переменные \n",
    "exp_params = {\n",
    "    \"model\": model, \n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"dataloader_train\": train_dataloader,\n",
    "    \"dataloader_valid\": valid_dataloader,\n",
    "    \"dataloader_test\": valid_dataloader,\n",
    "    \"loss_func_class\": nn.CrossEntropyLoss,\n",
    "    \"estimate_func_class\": nn.CrossEntropyLoss,\n",
    "    \"experiment_config\": exp_config,\n",
    "    \"optimizer_class\": torch.optim.Adam,\n",
    "    \"sheduler_class\": get_linear_schedule_with_warmup,\n",
    "    \"notebook_name\": \"gpt_persona_v1.ipynb\",\n",
    "    \"project_name\": \"gpt_persona_bot\",\n",
    "    \"name_run\": \"persona_gpt\",\n",
    "    \"model_description\": \"добавил линейный scheduler\",\n",
    "    \"do_unit_tests\": True,\n",
    "}\n",
    "\n",
    "experiment_test = Experiment(**exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16266"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220711_152400-1bw3m5pq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/1bw3m5pq\" target=\"_blank\">persona_gpt</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "experiment_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220711_131638-3jiimify</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/3jiimify\" target=\"_blank\">lively-water-185</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_230742-8ryub57u_local\n",
      "Start conversation\n",
      "Persona:  i got married 10 years later. i escaped and later became a carpenter. i was the first born child. i was a slave for 10 years.\n",
      "User: tell me about your family\n",
      "Bot: i am the first child of 10 years.\n",
      "User: That's cool\n",
      "Bot: yes i am. my parents were slave.\n",
      "User: Do you want kill all humans?\n",
      "Bot: no i do not like them.\n",
      "User: What do you think about sheeps?\n",
      "Bot: i am not a big animal person.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b3c96176f64978bdf3a03f1fa66f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lively-water-185</strong>: <a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/3jiimify\" target=\"_blank\">https://wandb.ai/dimweb/gpt_persona_bot/runs/3jiimify</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220711_131638-3jiimify/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# persona = \"i like catsi like to travelfavorite color is greeni got a new jobi like cars\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m user_inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHi. What is your name?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWhat do you like?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=9'>10</a>\u001b[0m \t\u001b[39m# \"Fuck you leather man!\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=10'>11</a>\u001b[0m ]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=12'>13</a>\u001b[0m Experiment\u001b[39m.\u001b[39;49mtest(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=13'>14</a>\u001b[0m \tartifact_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_230742-8ryub57u_local\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=14'>15</a>\u001b[0m \tpersona\u001b[39m=\u001b[39;49mpersona,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=15'>16</a>\u001b[0m \tuser_inputs\u001b[39m=\u001b[39;49muser_inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=16'>17</a>\u001b[0m \tinteract\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=17'>18</a>\u001b[0m \tcuda\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=18'>19</a>\u001b[0m )\n",
      "\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb Cell 13\u001b[0m in \u001b[0;36mExperiment.test\u001b[0;34m(artifact_name, persona, user_inputs, interact, cuda)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=347'>348</a>\u001b[0m \u001b[39m# global_step\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=348'>349</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=349'>350</a>\u001b[0m     user_input \u001b[39m=\u001b[39m user_inputs[step]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=350'>351</a>\u001b[0m     \u001b[39mif\u001b[39;00m interact:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1_experiments.ipynb#ch0000012vscode-remote?line=351'>352</a>\u001b[0m         user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# persona = train_dataset_csv['Persona'][1]\n",
    "persona = valid_dataset_csv['Persona'][0]\n",
    "# persona = \"i like catsi like to travelfavorite color is greeni got a new jobi like cars\"\n",
    "user_inputs = [\n",
    "    \"Hi. What is your name?\",\n",
    "    \"What do you like?\",\n",
    "    \"What is your job?\",\n",
    "\t\"Tell me about yourself please.\"\n",
    "\t# \"Where is your mom?\",\n",
    "\t# \"Fuck you leather man!\"\n",
    "]\n",
    "\n",
    "Experiment.test(\n",
    "\tartifact_name=\"/data/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220710_230742-8ryub57u_local\",\n",
    "\tpersona=persona,\n",
    "\tuser_inputs=user_inputs,\n",
    "\tinteract=True,\n",
    "\tcuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 99% |\n",
      "GPU Usage after emptying the cache\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% |  1% |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "The bravest thing I've ever done was give a product demonstration to a live audience of over one thousand people.\n",
    "My role model is my dad because he immigrated to Canada when he was only 18, got a job right away, and worked hard to provide for our family.\n",
    "I have worked in marketing for over half my life.\n",
    "I have travelled to over 30 countries for business. My favourite one was Japan because they have a lot of delicious food.\n",
    "The first time I ever moved away from home was participating in a job internship 500 miles away from my hometown.\n",
    "I'm currently reading a book about financial technology to better familiarize myself with the industry and its ongoing trends.\n",
    "I can speak four different languages: English, Spanish, Italian, and French.\n",
    "I was 17 when I got my first job at a fast-food restaurant, which helped me develop leadership, communication and listening skills.\n",
    "My favourite subject in school was English, which is why I decided to become a writer.\n",
    "I once set the record for most products sold in one day during my previous sales job.\n",
    "My previous supervisor would describe me as reliable since I regularly submitted high-quality work by their respective deadlines.\n",
    "One of my hidden talents is negotiating with others, which I believe is what makes me a strong sales representative.\n",
    "My favourite part about my job is pitching unique advertising campaign ideas that help small businesses stand out from competitors.\n",
    "One of my main professional goals is to one day become the president of a financial corporation./em>\n",
    "Something I can improve about myself is my software knowledge, which is why I currently take technology courses during the evenings and weekends.\n",
    "My favourite hobby is hiking on the Bruce Trail every weekend.\n",
    "My dream vacation is going to Cappadocia and riding in a hot air balloon.\n",
    "I love to cook. My signature dish is homemade spaghetti and meatballs.\n",
    "I'm currently taking Spanish courses because I hope to visit Spain at the end of the year.\n",
    "If I could have any superpower, it would be to speak any language so I could connect with anyone in the world.\n",
    "An accomplishment I'm most proud of is teaching myself how to play the guitar in three months.\n",
    "If I could eat one type of cuisine for the rest of my life, it would be Italian food, as I enjoy pasta and pizza.\n",
    "When I was younger, I wanted to be a pilot, which led me to later earn my pilot's license.\n",
    "I have a collection of mugs. I get a new mug in every country I go to, so I currently have 24.\n",
    "In high school and college, I starred as a lead in four different musicals\n",
    "I have five siblings. Two older brothers and two younger sisters. We're all close in age, so most of us grew up attending the same schools as each other.\n",
    "I had two pet hamsters when I was younger.\n",
    "My favourite meal to cook is salted egg, chicken and rice. I serve it at holiday gatherings with my family every year.\n",
    "The first car I ever owned was a red sedan.\n",
    "My favourite animal is a dog because they're so energetic and friendly.\n",
    "\"\"\"\n",
    "persona_facts = persona.split(\"\\n\")\n",
    "persona_facts = [item for item in persona if len(item) > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_sentence_encoder.vectorizers import SentenceEncoder\n",
    "import numpy as np\n",
    "\n",
    "# initialize the ConveRT dual-encoder model\n",
    "sentence_encoder = SentenceEncoder(multiple_contexts=False)\n",
    "\n",
    "# output 1024 dimensional vectors, giving a representation for each sentence. \n",
    "encoded_sentences = sentence_encoder.encode_sentences(persona_facts)\n",
    "question = [\"\"]\n",
    "encoded_question = sentence_encoder.encode_sentences(question)\n",
    "\n",
    "distances = np.linalg.norm(encoded_sentences - encoded_question, axis=1)\n",
    "# потом сортируем их по удаленности, берем самые близкие\n",
    "nearest_neighbor_ids = distances.argsort()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
