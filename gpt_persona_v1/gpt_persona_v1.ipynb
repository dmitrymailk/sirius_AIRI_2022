{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import importlib\n",
    "import os\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\n",
    "\t'<speaker_1>', \n",
    "\t'</speaker_1>', \n",
    "\t\n",
    "\t'<speaker_2>',\n",
    "\t'</speaker_2>',\n",
    "\t\n",
    "\t'<persona>',\n",
    "\t'</persona>'\n",
    "]\n",
    "\n",
    "SPECIAL_TOKENS = {item:item for item in SPECIAL_TOKENS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    def __init__(self, \n",
    "        model=None, \n",
    "        dataloader_train=None,\n",
    "        dataloader_valid=None,\n",
    "        dataloader_test=None,\n",
    "        loss_func_class=None,\n",
    "        estimate_func_class=None,\n",
    "        experiment_config=None,\n",
    "        optimizer_class=None,\n",
    "        sheduler_class=None,\n",
    "        project_name=None,\n",
    "        notebook_name=None,\n",
    "        name_run=\"\",\n",
    "        model_description=\"\"\n",
    "        ): \n",
    "        assert notebook_name != None, f\"notebook_name should be valid filename, but get {notebook_name}\"\n",
    "\n",
    "        # datasets\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        \n",
    "        # wandb\n",
    "        self.notebook_name = notebook_name\n",
    "        self.project_name = project_name \n",
    "        self.experiment_config = experiment_config\n",
    "        self.wandb_run = None\n",
    "        self.name_run = name_run\n",
    "        self.model_description = model_description\n",
    "        self.model_name = \"pytorch_model\"\n",
    "        self.model_artifact = None\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.sheduler_class = sheduler_class\n",
    "        self.loss_func_class = loss_func_class\n",
    "        self.estimate_func_class = estimate_func_class\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.sheduler = None\n",
    "        self.loss_func = None\n",
    "        self.estimate_func = None\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.device = torch.device('cpu')\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        # prepare for experiment\n",
    "        self.setup()\n",
    "        self.unit_tests()\n",
    "\n",
    "    def setup(self):\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.experiment_config['optimizer'])\n",
    "        \n",
    "        if self.sheduler_class != None:\n",
    "            self.sheduler = self.sheduler_class(self.optimizer, **self.experiment_config['sheduler'])\n",
    "\n",
    "        self.loss_func = self.loss_func_class()\n",
    "        self.estimate_func = self.estimate_func_class()\n",
    "\n",
    "        # set model name\n",
    "        date_time = self.get_date()\n",
    "        self.model_name = f\"{self.name_run}---{date_time}.pt\"\n",
    "        self.experiment_config['model_name'] = self.model_name\n",
    "\n",
    "        # setup wandb\n",
    "        # save model structure and weights to wandb\n",
    "        self.model_artifact = wandb.Artifact(\n",
    "            self.name_run, type=\"model\",\n",
    "            description=self.model_description,\n",
    "            metadata=self.experiment_config)\n",
    "\n",
    "\n",
    "    def get_date(self):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y__%H:%M:%S\")\n",
    "        return date_time\n",
    "\n",
    "    def unit_tests(self):\n",
    "        # test training\n",
    "        X, y = next(iter(self.dataloader_train))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "        pred = self.model(X)\n",
    "        pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        y = y[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = self.loss_func(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # test valid\n",
    "        X, y = next(iter(self.dataloader_valid))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        pred = self.model(X)\n",
    "        pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        y = y[..., 1:].contiguous().view(-1)\n",
    "        test_loss = self.estimate_func(pred, y).item()\n",
    "        # correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # initial validation\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += self.estimate_func(pred, y).item()\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        print(\"Initial val = \", test_loss)\n",
    "\n",
    "        print(\"tests ok\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=qrAWbBV1rd4I\n",
    "        # если попытаться создать переменную чтобы не городить тут код возникает ошибка с wandb!\n",
    "        with wandb.init(project=self.project_name, entity=\"dimweb\",\n",
    "                        settings=wandb.Settings(\n",
    "                            start_method=\"thread\", \n",
    "                            # symlink=False\n",
    "                            ),\n",
    "                        reinit=True,\n",
    "                        name=self.name_run,\n",
    "                        config=self.experiment_config,\n",
    "                        # sync_tensorboard=True\n",
    "                        ) as run:\n",
    "\n",
    "            self.run = run\n",
    "            \n",
    "            # save model class\n",
    "            self.save_model_class()\n",
    "\n",
    "            # start train\n",
    "            epochs = self.experiment_config['epochs']\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch: {i}\")\n",
    "                self.train_steps()\n",
    "                self.valid_steps()\n",
    "            \n",
    "            # sync model\n",
    "            self.wandb_save_model()\n",
    "            \n",
    "            print(f\"train end\")\n",
    "    \n",
    "    def save_model_class(self):\n",
    "        # save class\n",
    "        model_class_name = self.experiment_config['model_class_name']\n",
    "        class_script_path_dest = f\"{os.path.join(wandb.run.dir, model_class_name)}.py\"\n",
    "        class_script_path_src = f\"./models/{model_class_name}.py\"\n",
    "        shutil.copy2(class_script_path_src, class_script_path_dest)\n",
    "        self.model_artifact.add_file(class_script_path_dest)\n",
    "        wandb.save(class_script_path_dest)\n",
    "\n",
    "    def wandb_save_model(self):\n",
    "        # wandb использует symlinks для того чтобы сохранять файлы\n",
    "        # но из-за проблем с правами доступа возникает ошибка и модель нельзя сохранить\n",
    "        # поэтому пришлось сохранять модель в дирректорию с самим запуском\n",
    "        # https://docs.wandb.ai/guides/track/advanced/save-restore#example-of-saving-a-file-to-the-wandb-run-directory\n",
    "        model_save_path = os.path.join(wandb.run.dir, self.model_name)\n",
    "        torch.save(self.model.state_dict(), model_save_path)\n",
    "        self.model_artifact.add_file(model_save_path)\n",
    "        wandb.save(model_save_path)\n",
    "\n",
    "        # save notebook\n",
    "        notebook_path = os.path.join(wandb.run.dir, self.notebook_name)\n",
    "        shutil.copy2(self.notebook_name, notebook_path)\n",
    "        self.model_artifact.add_file(notebook_path)\n",
    "        wandb.save(notebook_path)\n",
    "\n",
    "        wandb.log_artifact(self.model_artifact)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        raise NotImplementedError(\"You need specify training steps\")\n",
    "\n",
    "    def valid_steps(self):\n",
    "        raise NotImplementedError(\"You need specify valid steps\")\n",
    "\n",
    "    def load_model(self, artifact_name, additional_model_args={}):\n",
    "        assert artifact_name != \"\"\n",
    "        with wandb.init(project=self.project_name, job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_path = os.path.join(model_dir, model_config['model_name'])\n",
    "            # print(model_config)\n",
    "\n",
    "            model_class_name = model_config['model_class_name']\n",
    "            model_script_path = f\"./artifacts/{artifact_name}/{model_class_name}.py\"\n",
    "            # get module by path https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path?rq=1 \n",
    "            model_class = getattr(SourceFileLoader(model_class_name, model_script_path).load_module(), model_class_name)\n",
    "            \n",
    "            model_args = model_config['model_args']\n",
    "            model = model_class(**model_args, **additional_model_args)\n",
    "\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            self.model = model\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_load_model(artifact_name=\"\", project_name=\"\", additional_model_args={}):\n",
    "        assert artifact_name != \"\"\n",
    "        assert project_name != \"\"\n",
    "        with wandb.init(project=project_name, job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_path = os.path.join(model_dir, model_config['model_name'])\n",
    "\n",
    "            model_class_name = model_config['model_class_name']\n",
    "            model_script_path = f\"./artifacts/{artifact_name}/{model_class_name}.py\" \n",
    "            model_class = getattr(SourceFileLoader(model_class_name, model_script_path).load_module(), model_class_name)\n",
    "            \n",
    "            model_args = model_config['model_args']\n",
    "            model = model_class(**model_args, **additional_model_args)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model.to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "            return model\n",
    "\n",
    "    def test(self, artifact_name=\"\", model=None):\n",
    "        raise NotImplementedError(\"You need specify test steps\")\n",
    "\n",
    "\n",
    "class Experiment(BaseExperiment):\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Experiment, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        self.model.train()\n",
    "        interval = self.experiment_config['check_interval']\n",
    "\n",
    "        for batch, (X, y) in enumerate(self.dataloader_train):\n",
    "            # Send data to training device\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_func(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.sheduler != None:\n",
    "                self.sheduler.step()\n",
    "            \n",
    "            # Progress output\n",
    "            if batch % interval == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "    def valid_steps(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                test_loss += self.estimate_func(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        \n",
    "        wandb.log({\"val_loss\": test_loss})\n",
    "        wandb.log({\"val_acc\": correct})\n",
    "\n",
    "    def test(self, artifact_name=\"\", model=None):\n",
    "        if model is None:\n",
    "            self.load_model(artifact_name)\n",
    "        else:\n",
    "            self.model = model\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        print(\"model loaded to disk\")\n",
    "        predictions = []\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, _ in self.test_dataloader:\n",
    "                X = X.to(self.device)\n",
    "                pred = self.model(X).argmax(1).cpu().numpy()\n",
    "                predictions.extend(list(pred))\n",
    "        \n",
    "        date_time = self.get_date()\n",
    "        filename = f\"./predictions/{self.name_run}---{date_time}.csv\"\n",
    "        with open(filename, 'w') as solution:\n",
    "            print('Id,Category', file=solution)\n",
    "            for i, label in enumerate(predictions):\n",
    "                print(f'{i},{label}', file=solution)\n",
    "        print(\"test end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Persona</th>\n",
       "      <th>chat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i like to remodel homes. i like to go hunting...</td>\n",
       "      <td>hi , how are you doing ? i am getting ready to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my mom is my best friend. i have four sisters...</td>\n",
       "      <td>hi , how are you doing today ?\\ni am spending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i had a gig at local theater last night. i wo...</td>\n",
       "      <td>we all live in a yellow submarine , a yellow s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Persona  \\\n",
       "0           0   i like to remodel homes. i like to go hunting...   \n",
       "1           1   my mom is my best friend. i have four sisters...   \n",
       "2           2   i had a gig at local theater last night. i wo...   \n",
       "\n",
       "                                                chat  \n",
       "0  hi , how are you doing ? i am getting ready to...  \n",
       "1  hi , how are you doing today ?\\ni am spending ...  \n",
       "2  we all live in a yellow submarine , a yellow s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n",
    "persona_chat_original = persona_chat_original[:1000]\n",
    "persona_chat_original.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatGenerator:\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.processed_dataset = []\n",
    "\t\tself.process_dataset()\n",
    "\n",
    "\tdef process_dataset(self):\n",
    "\t\tprocessed_dataset = {\n",
    "\t\t\t\"persona\": [],\n",
    "\t\t\t\"history\": [],\n",
    "\t\t\t# \"target\": []\n",
    "\t\t}\n",
    "\n",
    "\t\tspeaker_1_start = SPECIAL_TOKENS['<speaker_1>']\n",
    "\t\tspeaker_1_end = SPECIAL_TOKENS['</speaker_1>']\n",
    "\t\t\n",
    "\t\tspeaker_2_start = SPECIAL_TOKENS['<speaker_2>']\n",
    "\t\tspeaker_2_end = SPECIAL_TOKENS['</speaker_2>']\n",
    "\n",
    "\t\tfor i in range(len(self.initial_dataset)):\n",
    "\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n",
    "\t\t\tpersona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n",
    "\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n",
    "\t\t\tchat = chat[:-1]\n",
    "\t\t\thistory = \"\"\n",
    "\t\t\tfor j in range(len(chat)):\n",
    "\t\t\t\treply = chat[j]\n",
    "\t\t\t\tif (j+1) % 2 == 0:\n",
    "\t\t\t\t\treply = f\"{speaker_2_start} {reply} {speaker_2_end}\"\n",
    "\t\t\t\t\thistory += reply\n",
    "\n",
    "\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n",
    "\t\t\t\t\tprocessed_dataset['history'].append(history)\n",
    "\t\t\t\t\t# processed_dataset['target'].append(reply)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treply = f\"{speaker_1_start} {reply} {speaker_1_end}\"\n",
    "\t\t\t\t\thistory += reply \n",
    "\n",
    "\t\tdataset = pd.DataFrame(data=processed_dataset)\n",
    "\t\treturn dataset\n",
    "\n",
    "train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.1)\n",
    "train_dataset_csv, valid_dataset_csv = train_dataset_csv.reset_index(), valid_dataset_csv.reset_index()\n",
    "\n",
    "train_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=train_dataset_csv,\n",
    ")\n",
    "\n",
    "valid_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=valid_dataset_csv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatDataset(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.initial_dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.initial_dataset.iloc[idx]\n",
    "\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n",
    "\t\trandom.shuffle(persona)\n",
    "\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n",
    "\t\tpersona = torch.cat([*persona])\n",
    "\n",
    "\t\thistory = row['history']\n",
    "\t\thistory = self.tokenizer.encode(history)\n",
    "\t\thistory = torch.tensor(history).flatten()\n",
    "\t\t\n",
    "\t\t# target = row['target']\n",
    "\t\t# target = torch.tensor(self.tokenizer.encode(target)).flatten()\n",
    "\t\t# target = torch.cat([target, torch.tensor([self.tokenizer.eos_token_id], dtype=torch.long)])\n",
    "\n",
    "\t\t# feature = torch.cat([persona, history, torch.tensor([tokenizer.eos_token_id])])\n",
    "\t\tfeature = torch.cat([persona, history])\n",
    "\n",
    "\t\t# target = torch.tensor(target, dtype=torch.long)\n",
    "\t\t# target = target.flatten()\n",
    "\t\treturn {\n",
    "\t\t\t\"feature\": feature,\n",
    "\t\t\t\"target\": feature \n",
    "\t\t}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n",
    "\n",
    "train_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=train_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=valid_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "def collate(examples):\n",
    "\t# print(\"EXAMPLES\", examples)\n",
    "\tfeatures = [item['feature'] for item in examples]\n",
    "\tfeatures = pad_sequence(features, batch_first=True)\n",
    "\t\n",
    "\ttargets = [item['target'] for item in examples]\n",
    "\ttargets = pad_sequence(targets, batch_first=True)\n",
    "\n",
    "\t# return {\n",
    "\t# \t\"feature\": torch.tensor(features, dtype=torch.long),\n",
    "\t# \t\"target\": torch.tensor(targets, dtype=torch.long)\n",
    "\t# }\n",
    "\treturn torch.tensor(features, dtype=torch.long), torch.tensor(features, dtype=torch.long)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "\tbatch_size=4, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last = True,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "\tbatch_size=8, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last=False,\n",
    "\tshuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем dumb модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testt1\n",
      "Using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12023/2278523539.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(features, dtype=torch.long), torch.tensor(features, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial val =  10.83613044984879\n",
      "tests ok\n"
     ]
    }
   ],
   "source": [
    "# google colab не обновляет файлы, поэтому приходится делать это вручную, при помощи такого страшного импорта\n",
    "def import_class(class_name):\n",
    "    return getattr(importlib.reload(getattr(__import__(f\"models.{class_name}\"), class_name)), class_name)\n",
    "\n",
    "TestModel = import_class(\"TestModel\") \n",
    "\n",
    "\n",
    "exp_config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"check_interval\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-3\n",
    "    },\n",
    "    \"model_name\": \"pytorch_model\",\n",
    "    \"model_class_name\": str(TestModel.__name__),\n",
    "    \"model_args\": {\n",
    "        \"n_classes\": len(tokenizer)\n",
    "    }\n",
    "}\n",
    "\n",
    "exp_config[\"sheduler\"] = {\n",
    "    \"max_lr\": 0.01, \n",
    "    \"steps_per_epoch\": len(train_dataloader), \n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "model = TestModel(**exp_config['model_args'])\n",
    "model.test()\n",
    "\n",
    "# не хочу создавать глобальные переменные \n",
    "exp_params = {\n",
    "    \"model\": model, \n",
    "    \"dataloader_train\": train_dataloader,\n",
    "    \"dataloader_valid\": valid_dataloader,\n",
    "    \"dataloader_test\": valid_dataloader,\n",
    "    \"loss_func_class\": nn.CrossEntropyLoss,\n",
    "    \"estimate_func_class\": nn.CrossEntropyLoss,\n",
    "    \"experiment_config\": exp_config,\n",
    "    \"optimizer_class\": torch.optim.Adam,\n",
    "    \"sheduler_class\": None,\n",
    "    \"notebook_name\": \"gpt_persona_v1.ipynb\",\n",
    "    \"project_name\": \"gpt_persona_bot\",\n",
    "    \"name_run\": \"test_run\",\n",
    "    \"model_description\": \"Test my new model\",\n",
    "}\n",
    "\n",
    "experiment_test = Experiment(**exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.rand(\n",
    "\t(4, 123, 32),\n",
    "\tdtype=torch.float32,\n",
    ")\n",
    "# lin \n",
    "mat = torch.rand((32, 5623))\n",
    "a = torch.matmul(outputs, mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/DialoGPT-small were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50263, 768)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPTLM\n",
    "model = GPT2Model.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 208, 50263]) torch.Size([4, 208])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.1798, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(32, len(tokenizer))\n",
    "x = torch.randint(0, 1, (4, 208), dtype=torch.long)\n",
    "\n",
    "batch_size, sequence_length = x.shape[:2]\n",
    "hidden_states = torch.rand(\n",
    "\t(batch_size, sequence_length, 32),\n",
    "\tdtype=torch.float32,\n",
    ")\n",
    "logits = lin(hidden_states)\n",
    "print(logits.shape, x.shape)\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = x[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(\n",
    "\tshift_logits.view(-1, shift_logits.size(-1)), \n",
    "\tshift_labels.view(-1))\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2]) torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7992)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([\n",
    "\t[[3.4, 1], [1.5, 1],[0.4, 1], [0.10, 1]],\n",
    "\t[[3.4, 1], [1.5, 1],[0.4, 1], [0.10, 1]],\n",
    "\t[[3.4, 1], [1.5, 1],[0.4, 1], [0.10, 1]],\n",
    "],dtype=torch.float)\n",
    "target = torch.tensor([\n",
    "\t[0, 0],\n",
    "\t[0, 0],\n",
    "\t[0, 0],\n",
    "], dtype=torch.long)\n",
    "print(input.shape, target.shape)\n",
    "criteria(input, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
