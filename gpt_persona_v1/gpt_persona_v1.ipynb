{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import importlib\n",
    "import os\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\n",
    "\t'<speaker_1>', \n",
    "\t'</speaker_1>', \n",
    "\t\n",
    "\t'<speaker_2>',\n",
    "\t'</speaker_2>',\n",
    "\t\n",
    "\t'<persona>',\n",
    "\t'</persona>'\n",
    "]\n",
    "\n",
    "SPECIAL_TOKENS = {item:item for item in SPECIAL_TOKENS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseExperiment:\n",
    "    def __init__(self, \n",
    "        model=None, \n",
    "        tokenizer=None, \n",
    "        dataloader_train=None,\n",
    "        dataloader_valid=None,\n",
    "        dataloader_test=None,\n",
    "        loss_func_class=None,\n",
    "        estimate_func_class=None,\n",
    "        experiment_config=None,\n",
    "        optimizer_class=None,\n",
    "        sheduler_class=None,\n",
    "        project_name=None,\n",
    "        notebook_name=None,\n",
    "        name_run=\"\",\n",
    "        model_description=\"\",\n",
    "        do_unit_tests=True\n",
    "        ): \n",
    "        assert notebook_name != None, f\"notebook_name should be valid filename, but get {notebook_name}\"\n",
    "\n",
    "        # datasets\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        \n",
    "        # wandb\n",
    "        self.notebook_name = notebook_name\n",
    "        self.project_name = project_name \n",
    "        self.experiment_config = experiment_config\n",
    "        self.wandb_run = None\n",
    "        self.name_run = name_run\n",
    "        self.model_description = model_description\n",
    "        self.model_name = \"pytorch_model\"\n",
    "        self.pure_model_name = \"pytorch_model\"\n",
    "        self.model_artifact = None\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.sheduler_class = sheduler_class\n",
    "        self.loss_func_class = loss_func_class\n",
    "        self.estimate_func_class = estimate_func_class\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = None\n",
    "        self.sheduler = None\n",
    "        self.loss_func = None\n",
    "        self.estimate_func = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.device = torch.device('cpu')\n",
    "        print(f\"Using device {self.device}\")\n",
    "\n",
    "        # prepare for experiment\n",
    "        self.setup()\n",
    "        if do_unit_tests:\n",
    "            self.unit_tests()\n",
    "\n",
    "    def setup(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.transformer.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.optimizer = self.optimizer_class(self.model.parameters(), **self.experiment_config['optimizer'])\n",
    "        \n",
    "        if self.sheduler_class != None:\n",
    "            self.sheduler = self.sheduler_class(self.optimizer, **self.experiment_config['sheduler'])\n",
    "\n",
    "        self.loss_func = self.loss_func_class()\n",
    "        self.estimate_func = self.estimate_func_class()\n",
    "\n",
    "        # set model name\n",
    "        date_time = self.get_date()\n",
    "        self.model_name = f\"{self.name_run}---{date_time}.pt\"\n",
    "        self.pure_model_name = f\"{self.name_run}---{date_time}\"\n",
    "\n",
    "        self.experiment_config['model_name'] = self.model_name\n",
    "        self.experiment_config['pure_model_name'] = self.pure_model_name\n",
    "\n",
    "        # setup wandb\n",
    "        # save model structure and weights to wandb\n",
    "        self.model_artifact = wandb.Artifact(\n",
    "            self.name_run, type=\"model\",\n",
    "            description=self.model_description,\n",
    "            metadata=self.experiment_config)\n",
    "\n",
    "\n",
    "    def get_date(self):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y__%H:%M:%S\")\n",
    "        return date_time\n",
    "\n",
    "    def unit_tests(self):\n",
    "        # test training\n",
    "        X, y = next(iter(self.dataloader_train))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "        pred = self.model(X)\n",
    "        pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        y = y[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = self.loss_func(pred, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # test valid\n",
    "        X, y = next(iter(self.dataloader_valid))\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        pred = self.model(X)\n",
    "        pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "        y = y[..., 1:].contiguous().view(-1)\n",
    "        test_loss = self.estimate_func(pred, y).item()\n",
    "        # correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # initial validation\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += self.estimate_func(pred, y).item()\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        print(\"Initial val = \", test_loss)\n",
    "\n",
    "        print(\"tests ok\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=qrAWbBV1rd4I\n",
    "        # если попытаться создать переменную чтобы не городить тут код возникает ошибка с wandb!\n",
    "        with wandb.init(project=self.project_name, entity=\"dimweb\",\n",
    "                        settings=wandb.Settings(\n",
    "                            start_method=\"thread\", \n",
    "                            # symlink=False\n",
    "                            ),\n",
    "                        reinit=True,\n",
    "                        name=self.name_run,\n",
    "                        config=self.experiment_config,\n",
    "                        # sync_tensorboard=True\n",
    "                        ) as run:\n",
    "\n",
    "            self.run = run\n",
    "            \n",
    "            # save model class\n",
    "            self.save_model_class()\n",
    "\n",
    "            # start train\n",
    "            epochs = self.experiment_config['epochs']\n",
    "            for i in range(epochs):\n",
    "                print(f\"Epoch: {i}\")\n",
    "                self.train_steps()\n",
    "                self.valid_steps()\n",
    "            \n",
    "            # sync model\n",
    "            self.wandb_save_model()\n",
    "            \n",
    "            print(f\"train end\")\n",
    "    \n",
    "    def save_model_class(self):\n",
    "        # save class\n",
    "        model_class_name = self.experiment_config['model_class_name']\n",
    "        class_script_path_dest = f\"{os.path.join(wandb.run.dir, model_class_name)}.py\"\n",
    "        class_script_path_src = f\"./models/{model_class_name}.py\"\n",
    "        shutil.copy2(class_script_path_src, class_script_path_dest)\n",
    "        self.model_artifact.add_file(class_script_path_dest)\n",
    "        wandb.save(class_script_path_dest)\n",
    "\n",
    "    def wandb_save_model(self):\n",
    "        # wandb использует symlinks для того чтобы сохранять файлы\n",
    "        # но из-за проблем с правами доступа возникает ошибка и модель нельзя сохранить\n",
    "        # поэтому пришлось сохранять модель в дирректорию с самим запуском\n",
    "        # https://docs.wandb.ai/guides/track/advanced/save-restore#example-of-saving-a-file-to-the-wandb-run-directory\n",
    "        # model_save_path = os.path.join(wandb.run.dir, self.model_name)\n",
    "        # optimizer_save_path = os.path.join(wandb.run.dir, f\"optimizer.pt\")\n",
    "        # sheduler_save_path = os.path.join(wandb.run.dir, f\"scheduler.pt\")\n",
    "\n",
    "        self.model.save_pretrained(wandb.run.dir)\n",
    "        self.tokenizer.save_pretrained(wandb.run.dir)\n",
    "        \n",
    "        # torch.save(self.model.state_dict(), model_save_path)\n",
    "        # torch.save(self.optimizer.state_dict(), optimizer_save_path)\n",
    "        # torch.save(self.sheduler.state_dict(), sheduler_save_path)\n",
    "\n",
    "        # self.model_artifact.add_file(model_save_path)\n",
    "        # self.model_artifact.add_file(optimizer_save_path)\n",
    "        # self.model_artifact.add_file(sheduler_save_path)\n",
    "\n",
    "        # wandb.save(model_save_path)\n",
    "        # wandb.save(optimizer_save_path)\n",
    "        # wandb.save(sheduler_save_path)\n",
    "\n",
    "        self.experiment_config['saved_path'] = wandb.run.dir\n",
    "\n",
    "        # save notebook\n",
    "        notebook_path = os.path.join(wandb.run.dir, self.notebook_name)\n",
    "        shutil.copy2(self.notebook_name, notebook_path)\n",
    "        self.model_artifact.add_file(notebook_path)\n",
    "        wandb.save(notebook_path)\n",
    "\n",
    "        wandb.log_artifact(self.model_artifact)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        raise NotImplementedError(\"You need specify training steps\")\n",
    "\n",
    "    def valid_steps(self):\n",
    "        raise NotImplementedError(\"You need specify valid steps\")\n",
    "\n",
    "    def load_model(self, artifact_name, additional_model_args={}):\n",
    "        assert artifact_name != \"\"\n",
    "        with wandb.init(project=self.project_name, job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_path = os.path.join(model_dir, model_config['model_name'])\n",
    "            # print(model_config)\n",
    "\n",
    "            model_class_name = model_config['model_class_name']\n",
    "            model_script_path = f\"./artifacts/{artifact_name}/{model_class_name}.py\"\n",
    "            # get module by path https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path?rq=1 \n",
    "            model_class = getattr(SourceFileLoader(model_class_name, model_script_path).load_module(), model_class_name)\n",
    "            \n",
    "            model_args = model_config['model_args']\n",
    "            model = model_class(**model_args, **additional_model_args)\n",
    "\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            self.model = model\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_load_model(artifact_name=\"\", project_name=\"\", additional_model_args={}):\n",
    "        assert artifact_name != \"\"\n",
    "        assert project_name != \"\"\n",
    "        with wandb.init(project=project_name, job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_path = os.path.join(model_dir, model_config['model_name'])\n",
    "\n",
    "            model_class_name = model_config['model_class_name']\n",
    "            model_script_path = f\"./artifacts/{artifact_name}/{model_class_name}.py\" \n",
    "            model_class = getattr(SourceFileLoader(model_class_name, model_script_path).load_module(), model_class_name)\n",
    "            \n",
    "            model_args = model_config['model_args']\n",
    "            model = model_class(**model_args, **additional_model_args)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model.to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "            return model\n",
    "\n",
    "    def test(self, artifact_name=\"\", model=None):\n",
    "        raise NotImplementedError(\"You need specify test steps\")\n",
    "\n",
    "\n",
    "class Experiment(BaseExperiment):\n",
    "    def __init__(self, **kwargs): \n",
    "        super(Experiment, self).__init__(**kwargs)\n",
    "    \n",
    "    def train_steps(self):\n",
    "        self.model.train()\n",
    "        interval = self.experiment_config['check_interval']\n",
    "\n",
    "        for batch, (X, y) in enumerate(self.dataloader_train):\n",
    "            # Send data to training device\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Compute prediction error\n",
    "            pred = self.model(X)\n",
    "            pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "            y = y[..., 1:].contiguous().view(-1)\n",
    "            loss = self.loss_func(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.sheduler != None:\n",
    "                self.sheduler.step()\n",
    "            \n",
    "            # Progress output\n",
    "            if batch % interval == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "    def valid_steps(self):\n",
    "        self.model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        num_batches = len(self.dataloader_valid)\n",
    "        size = len(self.dataloader_valid.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader_valid:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                pred = pred[..., :-1, :].contiguous().view(-1, pred.size(-1))\n",
    "                y = y[..., 1:].contiguous().view(-1)\n",
    "                test_loss += self.estimate_func(pred, y).item()\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        # correct /= size\n",
    "        \n",
    "        wandb.log({\"val_loss\": test_loss})\n",
    "        # wandb.log({\"val_acc\": correct})\n",
    "\n",
    "    def test(self, artifact_name=\"\", persona=\"\", user_inputs=None):\n",
    "        with wandb.init(project=\"gpt_persona_bot\", job_type=\"inference\"):\n",
    "            model_artifact = wandb.use_artifact(artifact_name)\n",
    "            model_dir = model_artifact.download()\n",
    "            model_config = model_artifact.metadata\n",
    "            model_folder = model_config['saved_path'] \n",
    "            print(model_config)\n",
    "\n",
    "            model_class_name = model_config['model_class_name']\n",
    "            model_script_path = f\"./artifacts/{artifact_name}/{model_class_name}.py\"\n",
    "            # get module by path https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path?rq=1 \n",
    "            model_class = getattr(SourceFileLoader(model_class_name, model_script_path).load_module(), model_class_name)\n",
    "            \n",
    "            self.model = AutoModel.from_pretrained(model_folder)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_folder)\n",
    "            print(\"Start conversation\")\n",
    "            print(persona)\n",
    "            persona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n",
    "            chat_history_ids = self.tokenizer.encode(persona, return_tensors='pt')\n",
    "            VOCAB_TOKENS = self.tokenizer.get_added_vocab()\n",
    "\n",
    "            last_index = 0\n",
    "\n",
    "            for step in range(len(user_inputs)):\n",
    "                print(f\"User: {user_inputs[step]}\")\n",
    "\n",
    "                user_input = f\"{SPECIAL_TOKENS['<speaker_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</speaker_1>']}{SPECIAL_TOKENS['<speaker_2>']}\"\n",
    "                new_user_input_ids = self.tokenizer.encode(user_input, return_tensors='pt')\n",
    "                bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "\n",
    "                # generated a response while limiting the total chat history to 1000 tokens, \n",
    "                model_response = self.model.generate(\n",
    "                    bot_input_ids, max_length=250,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,  \n",
    "                    no_repeat_ngram_size=3,       \n",
    "                    do_sample=True, \n",
    "                    top_k=100,\n",
    "                    top_p=0.7,\n",
    "                    temperature = 0.8,\n",
    "                )\n",
    "\n",
    "                model_response_list = list(model_response[0])\n",
    "                end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS[\"</speaker_2>\"]) +1\n",
    "                last_index = end_speaker_index\n",
    "                model_response = model_response[:, :end_speaker_index+1]\n",
    "\n",
    "                chat_history_ids = model_response\n",
    "                print(f\"Bot: {self.tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True)}\")\n",
    "\n",
    "                print(\"test end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Persona</th>\n",
       "      <th>chat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i like to remodel homes. i like to go hunting...</td>\n",
       "      <td>hi , how are you doing ? i am getting ready to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my mom is my best friend. i have four sisters...</td>\n",
       "      <td>hi , how are you doing today ?\\ni am spending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i had a gig at local theater last night. i wo...</td>\n",
       "      <td>we all live in a yellow submarine , a yellow s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Persona  \\\n",
       "0           0   i like to remodel homes. i like to go hunting...   \n",
       "1           1   my mom is my best friend. i have four sisters...   \n",
       "2           2   i had a gig at local theater last night. i wo...   \n",
       "\n",
       "                                                chat  \n",
       "0  hi , how are you doing ? i am getting ready to...  \n",
       "1  hi , how are you doing today ?\\ni am spending ...  \n",
       "2  we all live in a yellow submarine , a yellow s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n",
    "persona_chat_original = persona_chat_original[:16]\n",
    "persona_chat_original.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatGenerator:\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.processed_dataset = []\n",
    "\t\tself.process_dataset()\n",
    "\n",
    "\tdef process_dataset(self):\n",
    "\t\tprocessed_dataset = {\n",
    "\t\t\t\"persona\": [],\n",
    "\t\t\t\"history\": [],\n",
    "\t\t\t# \"target\": []\n",
    "\t\t}\n",
    "\n",
    "\t\tspeaker_1_start = SPECIAL_TOKENS['<speaker_1>']\n",
    "\t\tspeaker_1_end = SPECIAL_TOKENS['</speaker_1>']\n",
    "\t\t\n",
    "\t\tspeaker_2_start = SPECIAL_TOKENS['<speaker_2>']\n",
    "\t\tspeaker_2_end = SPECIAL_TOKENS['</speaker_2>']\n",
    "\n",
    "\t\tfor i in range(len(self.initial_dataset)):\n",
    "\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n",
    "\t\t\tpersona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n",
    "\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n",
    "\t\t\tchat = chat[:-1]\n",
    "\t\t\thistory = \"\"\n",
    "\t\t\tfor j in range(len(chat)):\n",
    "\t\t\t\treply = chat[j]\n",
    "\t\t\t\tif (j+1) % 2 == 0:\n",
    "\t\t\t\t\treply = f\"{speaker_2_start} {reply} {speaker_2_end}\"\n",
    "\t\t\t\t\thistory += reply\n",
    "\n",
    "\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n",
    "\t\t\t\t\tprocessed_dataset['history'].append(history)\n",
    "\t\t\t\t\t# processed_dataset['target'].append(reply)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treply = f\"{speaker_1_start} {reply} {speaker_1_end}\"\n",
    "\t\t\t\t\thistory += reply \n",
    "\n",
    "\t\tdataset = pd.DataFrame(data=processed_dataset)\n",
    "\t\treturn dataset\n",
    "\n",
    "train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.1)\n",
    "train_dataset_csv, valid_dataset_csv = train_dataset_csv.reset_index(), valid_dataset_csv.reset_index()\n",
    "\n",
    "train_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=train_dataset_csv,\n",
    ")\n",
    "\n",
    "valid_dataset_generator = PersonaChatGenerator(\n",
    "\tinitial_dataset=valid_dataset_csv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaChatDataset(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tinitial_dataset=None,\n",
    "\t\ttokenizer=None\n",
    "\t):\n",
    "\t\tself.initial_dataset = initial_dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.initial_dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.initial_dataset.iloc[idx]\n",
    "\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n",
    "\t\trandom.shuffle(persona)\n",
    "\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n",
    "\t\tpersona = torch.cat([*persona])\n",
    "\n",
    "\t\thistory = row['history']\n",
    "\t\thistory = self.tokenizer.encode(history)\n",
    "\t\thistory = torch.tensor(history).flatten()\n",
    "\t\t\n",
    "\t\t# target = row['target']\n",
    "\t\t# target = torch.tensor(self.tokenizer.encode(target)).flatten()\n",
    "\t\t# target = torch.cat([target, torch.tensor([self.tokenizer.eos_token_id], dtype=torch.long)])\n",
    "\n",
    "\t\t# feature = torch.cat([persona, history, torch.tensor([tokenizer.eos_token_id])])\n",
    "\t\tfeature = torch.cat([persona, history])\n",
    "\n",
    "\t\t# target = torch.tensor(target, dtype=torch.long)\n",
    "\t\t# target = target.flatten()\n",
    "\t\treturn {\n",
    "\t\t\t\"feature\": feature,\n",
    "\t\t\t\"target\": feature \n",
    "\t\t}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n",
    "\n",
    "train_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=train_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "valid_dataset = PersonaChatDataset(\n",
    "\tinitial_dataset=valid_dataset_generator.process_dataset(),\n",
    "\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "def collate(examples):\n",
    "\t# print(\"EXAMPLES\", examples)\n",
    "\tfeatures = [item['feature'] for item in examples]\n",
    "\tfeatures = pad_sequence(features, batch_first=True)\n",
    "\t\n",
    "\ttargets = [item['target'] for item in examples]\n",
    "\ttargets = pad_sequence(targets, batch_first=True)\n",
    "\n",
    "\t# return {\n",
    "\t# \t\"feature\": torch.tensor(features, dtype=torch.long),\n",
    "\t# \t\"target\": torch.tensor(targets, dtype=torch.long)\n",
    "\t# }\n",
    "\treturn torch.tensor(features, dtype=torch.long), torch.tensor(features, dtype=torch.long)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "\tbatch_size=4, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last = True,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "\tbatch_size=8, \n",
    "\tcollate_fn=collate, \n",
    "\tdrop_last=False,\n",
    "\tshuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем dumb модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/DialoGPT-small were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# google colab не обновляет файлы, поэтому приходится делать это вручную, при помощи такого страшного импорта\n",
    "def import_class(class_name):\n",
    "    return getattr(importlib.reload(getattr(__import__(f\"models.{class_name}\"), class_name)), class_name)\n",
    "\n",
    "GPT2LMModel = import_class(\"GPT2LMModel\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n",
    "\n",
    "exp_config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"check_interval\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-3\n",
    "    },\n",
    "    \"model_name\": \"pytorch_model\",\n",
    "    \"model_class_name\": str(GPT2LMModel.__name__),\n",
    "    \"model_args\": {\n",
    "        \"n_classes\": len(tokenizer),\n",
    "        \"base_model_name\": \"microsoft/DialoGPT-small\",\n",
    "        \"emb_size\": 768\n",
    "    },\n",
    "    \"saved_path\": \"\"\n",
    "}\n",
    "\n",
    "exp_config[\"sheduler\"] = {\n",
    "    \"max_lr\": 0.01, \n",
    "    \"steps_per_epoch\": len(train_dataloader), \n",
    "    \"epochs\": 1\n",
    "}\n",
    "\n",
    "model = GPT2LMModel(**exp_config['model_args'])\n",
    "\n",
    "# не хочу создавать глобальные переменные \n",
    "exp_params = {\n",
    "    \"model\": model, \n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"dataloader_train\": train_dataloader,\n",
    "    \"dataloader_valid\": valid_dataloader,\n",
    "    \"dataloader_test\": valid_dataloader,\n",
    "    \"loss_func_class\": nn.CrossEntropyLoss,\n",
    "    \"estimate_func_class\": nn.CrossEntropyLoss,\n",
    "    \"experiment_config\": exp_config,\n",
    "    \"optimizer_class\": torch.optim.Adam,\n",
    "    \"sheduler_class\": None,\n",
    "    \"notebook_name\": \"gpt_persona_v1.ipynb\",\n",
    "    \"project_name\": \"gpt_persona_bot\",\n",
    "    \"name_run\": \"test_run\",\n",
    "    \"model_description\": \"Test my new model\",\n",
    "    \"do_unit_tests\": False\n",
    "}\n",
    "\n",
    "experiment_test = Experiment(**exp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "added_tokens.json     merges.txt\t       tokenizer_config.json\n",
      "config.json\t      output.log\t       tokenizer.json\n",
      "config.yaml\t      pytorch_model.bin        vocab.json\n",
      "GPT2LMModel.py\t      requirements.txt\t       wandb-metadata.json\n",
      "gpt_persona_v1.ipynb  special_tokens_map.json  wandb-summary.json\n"
     ]
    }
   ],
   "source": [
    "!ls /home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_034230-3rmqo56x/files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_093614-30t34ds8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/30t34ds8\" target=\"_blank\">expert-bee-7</a></strong> to <a href=\"https://wandb.ai/dimweb/gpt_persona_bot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1, 'sheduler': {'epochs': 1, 'max_lr': 0.01, 'steps_per_epoch': 25}, 'optimizer': {'lr': 0.001}, 'batch_size': 4, 'model_args': {'emb_size': 768, 'n_classes': 50263, 'base_model_name': 'microsoft/DialoGPT-small'}, 'model_name': 'test_run---07_08_2022__03:42:21.pt', 'saved_path': '/home/dimweb/sandbox/persona_bot/gpt_persona_v1/wandb/run-20220708_034230-3rmqo56x/files', 'check_interval': 100, 'pure_model_name': 'test_run---07_08_2022__03:42:21', 'model_class_name': 'GPT2LMModel'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b46cac4fae48cca5b5cea28c43904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">expert-bee-7</strong>: <a href=\"https://wandb.ai/dimweb/gpt_persona_bot/runs/30t34ds8\" target=\"_blank\">https://wandb.ai/dimweb/gpt_persona_bot/runs/30t34ds8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220708_093614-30t34ds8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2Model:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50263, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000014vscode-remote?line=0'>1</a>\u001b[0m experiment_test\u001b[39m.\u001b[39;49mtest(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000014vscode-remote?line=1'>2</a>\u001b[0m \tartifact_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest_run:v2\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000014vscode-remote?line=2'>3</a>\u001b[0m )\n",
      "\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb Cell 4'\u001b[0m in \u001b[0;36mExperiment.test\u001b[0;34m(self, artifact_name, persona, user_inputs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000003vscode-remote?line=322'>323</a>\u001b[0m \u001b[39m# get module by path https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path?rq=1 \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000003vscode-remote?line=323'>324</a>\u001b[0m model_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(SourceFileLoader(model_class_name, model_script_path)\u001b[39m.\u001b[39mload_module(), model_class_name)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000003vscode-remote?line=325'>326</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_folder)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000003vscode-remote?line=326'>327</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_folder)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona_v1/gpt_persona_v1.ipynb#ch0000003vscode-remote?line=327'>328</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart conversation\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    445\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    447\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2222\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2223\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m-> 2225\u001b[0m     model, missing_keys, unexpected_keys, mismatched_keys, error_msgs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2226\u001b[0m         model,\n\u001b[1;32m   2227\u001b[0m         state_dict,\n\u001b[1;32m   2228\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2229\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2230\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2231\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2232\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2233\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2234\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2235\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2236\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2237\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2238\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2239\u001b[0m     )\n\u001b[1;32m   2241\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   2242\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2470\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   2467\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   2468\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2469\u001b[0m         )\n\u001b[0;32m-> 2470\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unexpected_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2473\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   2474\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSome weights of the model checkpoint at \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m were not used when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2475\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m initializing \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00munexpected_keys\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m- This IS expected if you are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2481\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2Model:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50263, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "experiment_test.test(\n",
    "\tartifact_name=\"test_run:v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2LMModel = import_class(\"GPT2LMModel\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n",
    "model = GPT2LMModel(\n",
    "\tbase_model_name=\"microsoft/DialoGPT-small\", \n",
    "\tn_classes=len(tokenizer), \n",
    "\temb_size=768\n",
    ")\n",
    "model.transformer.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.save_pretrained(\"./test\")\n",
    "tokenizer.save_pretrained(\"./test\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"./test\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "model = GPT2Model()\n",
    "model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
