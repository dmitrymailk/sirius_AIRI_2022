{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3146,"status":"ok","timestamp":1657176623308,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"i-_4ywQQFsOT","outputId":"dedee314-25ac-4e7f-b21b-850d66d6bcd1"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1657176623309,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"2rbRXz97FuJD","outputId":"d7472c73-65d4-44aa-d527-5e45032a72e4"},"outputs":[],"source":["# %cd drive/MyDrive/collab_sandbox/persona_bot_sirius/"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2782,"status":"ok","timestamp":1657176626082,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"77K05qg_F7zc"},"outputs":[],"source":["# ! pip -q install transformers"]},{"cell_type":"markdown","metadata":{"id":"IuzSROqxjUKM"},"source":["## Model initial configuration"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3542,"status":"ok","timestamp":1657176629617,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"g91QzdqU2haO"},"outputs":[],"source":["import glob\n","import logging\n","import os\n","import pickle\n","import random\n","import re\n","import shutil\n","import json\n","from typing import Dict, List, Tuple\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","from sklearn.model_selection import train_test_split\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","\n","from pathlib import Path\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","# Configs\n","logger = logging.getLogger(__name__)\n","\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":579,"status":"ok","timestamp":1657179873239,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"4GykNZzQFqK8"},"outputs":[],"source":["SPECIAL_TOKENS = [\n","\t '<speaker_1>', \n","\t '</speaker_1>', \n","\t \n","\t '<speaker_2>',\n","\t '</speaker_2>',\n","\n","\t '<persona>',\n","\t '</persona>'\n","]\n","SPECIAL_TOKENS = {item:item for item in SPECIAL_TOKENS}"]},{"cell_type":"markdown","metadata":{"id":"hkNfsIvbFqK9"},"source":["## Persona chat dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1657181800332,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"veW-25Q0FqK9","outputId":"be6ff1e2-5068-4225-af30-b9977f8abbc0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Persona</th>\n","      <th>chat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>i like to remodel homes. i like to go hunting...</td>\n","      <td>hi , how are you doing ? i am getting ready to...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>my mom is my best friend. i have four sisters...</td>\n","      <td>hi , how are you doing today ?\\ni am spending ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>i had a gig at local theater last night. i wo...</td>\n","      <td>we all live in a yellow submarine , a yellow s...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                            Persona  \\\n","0           0   i like to remodel homes. i like to go hunting...   \n","1           1   my mom is my best friend. i have four sisters...   \n","2           2   i had a gig at local theater last night. i wo...   \n","\n","                                                chat  \n","0  hi , how are you doing ? i am getting ready to...  \n","1  hi , how are you doing today ?\\ni am spending ...  \n","2  we all live in a yellow submarine , a yellow s...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n","persona_chat_original = persona_chat_original[:10]\n","persona_chat_original.head(3)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":534,"status":"ok","timestamp":1657181804840,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"Hwe1DC3FFqK-"},"outputs":[],"source":["class PersonaChatGenerator:\n","\tdef __init__(self, \n","\t\tinitial_dataset=None,\n","\t):\n","\t\tself.initial_dataset = initial_dataset\n","\t\tself.processed_dataset = []\n","\t\tself.process_dataset()\n","\n","\tdef process_dataset(self):\n","\t\tprocessed_dataset = {\n","\t\t\t\"persona\": [],\n","\t\t\t\"history\": [],\n","\t\t\t# \"target\": []\n","\t\t}\n","\n","\t\tspeaker_1_start = SPECIAL_TOKENS['<speaker_1>']\n","\t\tspeaker_1_end = SPECIAL_TOKENS['</speaker_1>']\n","\t\t\n","\t\tspeaker_2_start = SPECIAL_TOKENS['<speaker_2>']\n","\t\tspeaker_2_end = SPECIAL_TOKENS['</speaker_2>']\n","\n","\t\tfor i in range(len(self.initial_dataset)):\n","\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n","\t\t\tpersona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n","\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n","\t\t\tchat = chat[:-1]\n","\t\t\thistory = \"\"\n","\t\t\tfor j in range(len(chat)):\n","\t\t\t\treply = chat[j]\n","\t\t\t\tif (j+1) % 2 == 0:\n","\t\t\t\t\treply = f\"{speaker_2_start} {reply} {speaker_2_end}\"\n","\t\t\t\t\thistory += reply\n","\n","\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n","\t\t\t\t\tprocessed_dataset['history'].append(history)\n","\t\t\t\t\t# processed_dataset['target'].append(reply)\n","\n","\t\t\t\telse:\n","\t\t\t\t\treply = f\"{speaker_1_start} {reply} {speaker_1_end}\"\n","\t\t\t\t\thistory += reply \n","\n","\t\tdataset = pd.DataFrame(data=processed_dataset)\n","\t\treturn dataset\n","\n","train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.1)\n","\n","train_dataset_generator = PersonaChatGenerator(\n","\tinitial_dataset=train_dataset_csv,\n",")\n","\n","valid_dataset_generator = PersonaChatGenerator(\n","\tinitial_dataset=valid_dataset_csv,\n",")\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":12481,"status":"ok","timestamp":1657181820017,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"tdeGQaG6FqK_"},"outputs":[],"source":["class PersonaChatDataset(Dataset):\n","\tdef __init__(self, \n","\t\tinitial_dataset=None,\n","\t\ttokenizer=None\n","\t):\n","\t\tself.initial_dataset = initial_dataset\n","\t\tself.tokenizer = tokenizer\n","\t\n","\tdef __len__(self):\n","\t\treturn len(self.initial_dataset)\n","\t\n","\tdef __getitem__(self, idx):\n","\t\trow = self.initial_dataset.iloc[idx]\n","\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n","\t\trandom.shuffle(persona)\n","\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n","\t\tpersona = torch.cat([*persona])\n","\n","\t\thistory = row['history']\n","\t\thistory = self.tokenizer.encode(history)\n","\t\thistory = torch.tensor(history).flatten()\n","\t\t\n","\t\t# target = row['target']\n","\t\t# target = torch.tensor(self.tokenizer.encode(target)).flatten()\n","\t\t# target = torch.cat([target, torch.tensor([self.tokenizer.eos_token_id], dtype=torch.long)])\n","\n","\t\t# feature = torch.cat([persona, history, torch.tensor([tokenizer.eos_token_id])])\n","\t\tfeature = torch.cat([persona, history])\n","\n","\t\t# target = torch.tensor(target, dtype=torch.long)\n","\t\t# target = target.flatten()\n","\t\treturn {\n","\t\t\t\"feature\": feature,\n","\t\t\t\"target\": feature \n","\t\t}\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n","tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","\n","train_dataset = PersonaChatDataset(\n","\tinitial_dataset=train_dataset_generator.process_dataset(),\n","\ttokenizer=tokenizer\n",")\n","\n","valid_dataset = PersonaChatDataset(\n","\tinitial_dataset=valid_dataset_generator.process_dataset(),\n","\ttokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657181820019,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"utprDGf06OVt"},"outputs":[],"source":["# Args to allow for easy convertion of python script to notebook\n","class Args():\n","    def __init__(self):\n","        self.output_dir = 'output-small-test'\n","        self.model_type = 'gpt2'\n","        self.model_name_or_path = 'microsoft/DialoGPT-small'\n","        self.config_name = 'microsoft/DialoGPT-small'\n","        self.tokenizer_name = 'microsoft/DialoGPT-small'\n","        self.cache_dir = 'cached'\n","        self.block_size = 512\n","        self.do_train = True\n","        self.do_eval = True\n","        self.evaluate_during_training = False\n","        self.per_gpu_train_batch_size = 4\n","        self.per_gpu_eval_batch_size = 4\n","        self.gradient_accumulation_steps = 1\n","        self.learning_rate = 5e-5\n","        self.weight_decay = 0.0\n","        self.adam_epsilon = 1e-8\n","        self.max_grad_norm = 1.0\n","        self.num_train_epochs = 2\n","        self.max_steps = -1\n","        self.warmup_steps = 0\n","        self.logging_steps = 1000\n","        self.save_steps = 3500\n","        self.save_total_limit = None\n","        self.eval_all_checkpoints = False\n","        self.no_cuda = False\n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","        self.should_continue = False\n","        self.seed = 42\n","        self.local_rank = -1\n","        self.fp16 = False\n","        self.fp16_opt_level = 'O1'\n","\n","args = Args()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657181820020,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"naaRHoXgnStq"},"outputs":[],"source":["# Cacheing and storing of data/checkpoints\n","\n","def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n","    return valid_dataset if evaluate else train_dataset\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)"]},{"cell_type":"markdown","metadata":{"id":"pkvMNnrnVHQw"},"source":["## Training and Evaluating\n","\n","There will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format.\n","\n","Image from [Giphy](https://giphy.com/)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657181820020,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"tXzKlXHeu0Mb"},"outputs":[],"source":["def train(args: Args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size\n","\n","    def collate(examples):\n","        # print(\"EXAMPLES\", examples)\n","        features = [item['feature'] for item in examples]\n","        features = pad_sequence(features, batch_first=True)\n","        \n","        targets = [item['target'] for item in examples]\n","        targets = pad_sequence(targets, batch_first=True)\n","\n","        return {\n","            \"feature\": torch.tensor(features, dtype=torch.long),\n","            \"target\": torch.tensor(targets, dtype=torch.long)\n","        }\n","\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    # Train!\n","    global_step = 1\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            inputs, labels = (batch['feature'], batch['target'])\n","            # print(inputs.shape, labels.shape, inputs, labels)\n","            if inputs.shape[1] > 1024: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","        \n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            \n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if args.evaluate_during_training:\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","\n","                    # _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","# Evaluation of some model\n","\n","def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args.output_dir\n","\n","    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n","    os.makedirs(eval_output_dir, exist_ok=True)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size\n","    # Note that DistributedSampler samples randomly\n","\n","    def collate(examples: List[torch.Tensor]):\n","        features = [item['feature'] for item in examples]\n","        features = pad_sequence(features, batch_first=True)\n","        \n","        targets = [item['target'] for item in examples]\n","        targets = pad_sequence(targets, batch_first=True)\n","\n","        return {\n","            \"feature\": torch.tensor(features, dtype=torch.long),\n","            \"target\": torch.tensor(targets, dtype=torch.long)\n","        }\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    # Eval!\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = (batch['feature'], batch['target'])\n","        inputs = inputs.to(args.device)\n","        labels = labels.to(args.device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","            lm_loss = outputs[0]\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\"perplexity\": perplexity}\n","\n","    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        for key in sorted(result.keys()):\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return result"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657181822667,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"-MGD6bFXV4Z-"},"outputs":[],"source":["# Main runner\n","\n","def main(df_trn, df_val):\n","    args = Args()\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n","    tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","    \n","    model = AutoModelWithLMHead.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=False,\n","        config=config,\n","        cache_dir=args.cache_dir,\n","    )\n","    model.resize_token_embeddings(len(tokenizer))\n","    model.to(args.device)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n","\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    # ...\n","    results = {}\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"UZEHDzR0Vjs7"},"source":["It is time to train our model!"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["863122f18665429cab7e0da1f2d693ec","6eda16c62cc84d83a7527a3f5985e934","77721d2a1ecf453fa6718b20a130e9e5","909c0603d6894251a8fe0ba38d87fd2b","2a894417c73b43f38b3e0c3f120e3213","2f493defe2a24570bab122ed85152645","b4834894c3a3442abaeb707a3d16b140","bc669ca1b3144272ba2974ad6e69f20b","67f0fcae76bb4f0cad446bbc79d318f5","a58334a08d994881b2e35839efceabfc","7db62f83311a4d2db429b66350504b77","6f2ac9fc6de14488927c112b9896697b","42985a873c5649d19898d0a84ac74619","3387723669eb435da7956fb3b016104c","4aa98c116eb24e68a08a6dae89427e24","309c7738760d4f319b2a7294578d12b4","351523c10c804f52ab4869ece8bdf54f","93eb4ed831fd4739ab0b6e09ba6f3a45","868958dc0bb340628190bfacc1517198","cf9e8e25d557474cb38f9470ab757ebf","1a2326dd79a64dc7b56385255a9d68a7","1751d49229ae42fe8fca88a4a9f737c3"]},"id":"__iqR8YFV-Ex","outputId":"3296e725-b2d0-49b7-b970-ac03eba5aee0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dimweb/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:969: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n","/home/dimweb/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eca22afe32984299a079823e95f49b07","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30e50535489e4fd3a655d2bf95819082","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_22388/1749540694.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"feature\": torch.tensor(features, dtype=torch.long),\n","/tmp/ipykernel_22388/1749540694.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"target\": torch.tensor(targets, dtype=torch.long)\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 1.34 GiB already allocated; 6.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m main(train_dataset, valid_dataset)\n","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 15'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(df_trn, df_val)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=41'>42</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=43'>44</a>\u001b[0m     global_step, tr_loss \u001b[39m=\u001b[39m train(args, train_dataset, model, tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=44'>45</a>\u001b[0m \u001b[39m# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m# Create output directory if needed\u001b[39;00m\n","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=90'>91</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=91'>92</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=92'>93</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=93'>94</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# model outputs are always tuple in transformers (see doc)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=96'>97</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1048\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1048\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1049\u001b[0m     input_ids,\n\u001b[1;32m   1050\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1051\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1052\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1053\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1054\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1055\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1056\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1057\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1058\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1059\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1060\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1061\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1062\u001b[0m )\n\u001b[1;32m   1063\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:891\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    881\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    882\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    883\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[1;32m    893\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    894\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    895\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    896\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    898\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    899\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    900\u001b[0m     )\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    392\u001b[0m     hidden_states,\n\u001b[1;32m    393\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    394\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    395\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    396\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    397\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    398\u001b[0m )\n\u001b[1;32m    399\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:332\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    335\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:212\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[1;32m    211\u001b[0m attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mtype(value\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 212\u001b[0m attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_dropout(attn_weights)\n\u001b[1;32m    214\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 1.34 GiB already allocated; 6.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["main(train_dataset, valid_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1657179332948,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"Kzjw63Xeh60a","outputId":"bbefdce6-6088-4822-fc1b-92be33ab88e4"},"outputs":[{"data":{"text/plain":["' i like to smell my own farts. my beer gut is so huge i haven t seen my feet in two years. i am from san fransico. i am always the one who buys the beers. i like to place blame on other people even when i know it is my fault.'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["valid_dataset_csv = valid_dataset_csv.reset_index()\n","valid_dataset_csv['Persona'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"6eDkPEuvbD47"},"source":["## Chatting with  Bot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n","tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","model = AutoModelWithLMHead.from_pretrained('output-small')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21932,"status":"ok","timestamp":1657181731404,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"nIeqMwZktv7N","outputId":"c56f2656-10cf-4a97-ade3-ddb4a92c06b8"},"outputs":[{"name":"stdout","output_type":"stream","text":[" my favorite food is spaghetti and meatballs. i was raised by two mothers. i am not afraid of what others think. my boyfriend works for nasa. i can be quite forgetful.\n","Hi. What is your name?\n","Bot:  i am mark. nice to meet you \n","What do you like?\n","Bot:  pizza. i am a huge fan of spaghetti and italian meatballs \n","What is your job?\n","Bot:  my job is in the military. i have a degree in biology \n"]}],"source":["# valid_dataset_csv = valid_dataset_csv.reset_index()\n","persona = valid_dataset_csv.iloc[6]['Persona']\n","print(persona)\n","persona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n","chat_history_ids = tokenizer.encode(persona, return_tensors='pt')\n","VOCAB_TOKENS = tokenizer.get_added_vocab()\n","\n","user_inputs = [\n","    \"Hi. What is your name?\",\n","    \"What do you like?\",\n","    \"What is your job?\"\n","]\n","\n","last_index = 0\n","\n","for step in range(len(user_inputs)):\n","    # encode the new user input, add the eos_token and return a tensor in Pytorch\n","    # input('>> User:')\n","    print(user_inputs[step])\n","    user_input = f\"{SPECIAL_TOKENS['<speaker_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</speaker_1>']}{SPECIAL_TOKENS['<speaker_2>']}\"\n","    user_input\n","    new_user_input_ids = tokenizer.encode(user_input, return_tensors='pt')\n","    # print(new_user_input_ids)\n","\n","    # append the new user input tokens to the chat history\n","    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","\n","    # generated a response while limiting the total chat history to 1000 tokens, \n","    model_response = model.generate(\n","        bot_input_ids, max_length=250,\n","        pad_token_id=tokenizer.eos_token_id,  \n","        no_repeat_ngram_size=3,       \n","        do_sample=True, \n","        top_k=100,\n","        top_p=0.7,\n","        temperature = 0.8,\n","    )\n","\n","    # print(\"-\"*100)\n","    # print(tokenizer.decode(model_response[0]))\n","    # print(\"-\"*100)\n","    # print(list(model_response[0]), list(model_response[0]).index(VOCAB_TOKENS[\"</speaker_2>\"]))\n","    model_response_list = list(model_response[0])\n","    end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS[\"</speaker_2>\"]) +1\n","    last_index = end_speaker_index\n","    model_response = model_response[:, :end_speaker_index+1]\n","    # print(chat_history_ids, torch.tensor([[VOCAB_TOKENS['<speaker_2>']]], dtype=torch.long), model_response)\n","\n","    chat_history_ids = model_response\n","    # print(\"-\"*100)\n","    # print(tokenizer.decode(chat_history_ids[0]))\n","    # print(\"-\"*100)\n","    # chat_history_ids = torch.cat([\n","    #     chat_history_ids, \n","    #     torch.tensor([[VOCAB_TOKENS['</speaker_2>']]], dtype=torch.long), \n","    # ], dim=1)\n","    # print(model_response)\n","    # pretty print last ouput tokens from bot\n","    # print(chat_history_ids)\n","    print(f\"Bot: {tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"gpt_persona.ipynb","provenance":[{"file_id":"1NP986rQFTX9yI07fcvuHaZ5lVs9ya3l0","timestamp":1657101427139},{"file_id":"15wa925dj7jvdvrz8_z3vU7btqAFQLVlG","timestamp":1657100391790}]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"1751d49229ae42fe8fca88a4a9f737c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a2326dd79a64dc7b56385255a9d68a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a894417c73b43f38b3e0c3f120e3213":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f493defe2a24570bab122ed85152645":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309c7738760d4f319b2a7294578d12b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3387723669eb435da7956fb3b016104c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_868958dc0bb340628190bfacc1517198","max":2492,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf9e8e25d557474cb38f9470ab757ebf","value":1259}},"351523c10c804f52ab4869ece8bdf54f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42985a873c5649d19898d0a84ac74619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_351523c10c804f52ab4869ece8bdf54f","placeholder":"​","style":"IPY_MODEL_93eb4ed831fd4739ab0b6e09ba6f3a45","value":"Iteration:  51%"}},"4aa98c116eb24e68a08a6dae89427e24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a2326dd79a64dc7b56385255a9d68a7","placeholder":"​","style":"IPY_MODEL_1751d49229ae42fe8fca88a4a9f737c3","value":" 1259/2492 [05:56&lt;05:28,  3.75it/s]"}},"67f0fcae76bb4f0cad446bbc79d318f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6eda16c62cc84d83a7527a3f5985e934":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f493defe2a24570bab122ed85152645","placeholder":"​","style":"IPY_MODEL_b4834894c3a3442abaeb707a3d16b140","value":"Epoch:   0%"}},"6f2ac9fc6de14488927c112b9896697b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42985a873c5649d19898d0a84ac74619","IPY_MODEL_3387723669eb435da7956fb3b016104c","IPY_MODEL_4aa98c116eb24e68a08a6dae89427e24"],"layout":"IPY_MODEL_309c7738760d4f319b2a7294578d12b4"}},"77721d2a1ecf453fa6718b20a130e9e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc669ca1b3144272ba2974ad6e69f20b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67f0fcae76bb4f0cad446bbc79d318f5","value":0}},"7db62f83311a4d2db429b66350504b77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"863122f18665429cab7e0da1f2d693ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6eda16c62cc84d83a7527a3f5985e934","IPY_MODEL_77721d2a1ecf453fa6718b20a130e9e5","IPY_MODEL_909c0603d6894251a8fe0ba38d87fd2b"],"layout":"IPY_MODEL_2a894417c73b43f38b3e0c3f120e3213"}},"868958dc0bb340628190bfacc1517198":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"909c0603d6894251a8fe0ba38d87fd2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a58334a08d994881b2e35839efceabfc","placeholder":"​","style":"IPY_MODEL_7db62f83311a4d2db429b66350504b77","value":" 0/1 [00:00&lt;?, ?it/s]"}},"93eb4ed831fd4739ab0b6e09ba6f3a45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a58334a08d994881b2e35839efceabfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4834894c3a3442abaeb707a3d16b140":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc669ca1b3144272ba2974ad6e69f20b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9e8e25d557474cb38f9470ab757ebf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
