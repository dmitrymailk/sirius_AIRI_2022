{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3146,"status":"ok","timestamp":1657176623308,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"i-_4ywQQFsOT","outputId":"dedee314-25ac-4e7f-b21b-850d66d6bcd1"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1657176623309,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"2rbRXz97FuJD","outputId":"d7472c73-65d4-44aa-d527-5e45032a72e4"},"outputs":[],"source":["# %cd drive/MyDrive/collab_sandbox/persona_bot_sirius/"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2782,"status":"ok","timestamp":1657176626082,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"77K05qg_F7zc"},"outputs":[],"source":["# ! pip -q install transformers"]},{"cell_type":"markdown","metadata":{"id":"IuzSROqxjUKM"},"source":["## Model initial configuration"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3542,"status":"ok","timestamp":1657176629617,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"g91QzdqU2haO"},"outputs":[],"source":["import glob\n","import logging\n","import os\n","import pickle\n","import random\n","import re\n","import shutil\n","import json\n","from typing import Dict, List, Tuple\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","from sklearn.model_selection import train_test_split\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","\n","from pathlib import Path\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","# Configs\n","logger = logging.getLogger(__name__)\n","\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":579,"status":"ok","timestamp":1657179873239,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"4GykNZzQFqK8"},"outputs":[],"source":["SPECIAL_TOKENS = [\n","\t '<speaker_1>', \n","\t '</speaker_1>', \n","\t \n","\t '<speaker_2>',\n","\t '</speaker_2>',\n","\n","\t '<persona>',\n","\t '</persona>'\n","]\n","SPECIAL_TOKENS = {item:item for item in SPECIAL_TOKENS}"]},{"cell_type":"markdown","metadata":{"id":"hkNfsIvbFqK9"},"source":["## Persona chat dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1657181800332,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"veW-25Q0FqK9","outputId":"be6ff1e2-5068-4225-af30-b9977f8abbc0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Persona</th>\n","      <th>chat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>i like to remodel homes. i like to go hunting...</td>\n","      <td>hi , how are you doing ? i am getting ready to...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>my mom is my best friend. i have four sisters...</td>\n","      <td>hi , how are you doing today ?\\ni am spending ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>i had a gig at local theater last night. i wo...</td>\n","      <td>we all live in a yellow submarine , a yellow s...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                            Persona  \\\n","0           0   i like to remodel homes. i like to go hunting...   \n","1           1   my mom is my best friend. i have four sisters...   \n","2           2   i had a gig at local theater last night. i wo...   \n","\n","                                                chat  \n","0  hi , how are you doing ? i am getting ready to...  \n","1  hi , how are you doing today ?\\ni am spending ...  \n","2  we all live in a yellow submarine , a yellow s...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["persona_chat_original = pd.read_csv(\"./persona_chat.csv\")\n","persona_chat_original = persona_chat_original[:10]\n","persona_chat_original.head(3)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":534,"status":"ok","timestamp":1657181804840,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"Hwe1DC3FFqK-"},"outputs":[],"source":["class PersonaChatGenerator:\n","\tdef __init__(self, \n","\t\tinitial_dataset=None,\n","\t):\n","\t\tself.initial_dataset = initial_dataset\n","\t\tself.processed_dataset = []\n","\t\tself.process_dataset()\n","\n","\tdef process_dataset(self):\n","\t\tprocessed_dataset = {\n","\t\t\t\"persona\": [],\n","\t\t\t\"history\": [],\n","\t\t\t# \"target\": []\n","\t\t}\n","\n","\t\tspeaker_1_start = SPECIAL_TOKENS['<speaker_1>']\n","\t\tspeaker_1_end = SPECIAL_TOKENS['</speaker_1>']\n","\t\t\n","\t\tspeaker_2_start = SPECIAL_TOKENS['<speaker_2>']\n","\t\tspeaker_2_end = SPECIAL_TOKENS['</speaker_2>']\n","\n","\t\tfor i in range(len(self.initial_dataset)):\n","\t\t\tpersona = self.initial_dataset['Persona'].iloc[i]\n","\t\t\tpersona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n","\t\t\tchat = self.initial_dataset['chat'].iloc[i].split(\"\\n\")\n","\t\t\tchat = chat[:-1]\n","\t\t\thistory = \"\"\n","\t\t\tfor j in range(len(chat)):\n","\t\t\t\treply = chat[j]\n","\t\t\t\tif (j+1) % 2 == 0:\n","\t\t\t\t\treply = f\"{speaker_2_start} {reply} {speaker_2_end}\"\n","\t\t\t\t\thistory += reply\n","\n","\t\t\t\t\tprocessed_dataset['persona'].append(persona)\n","\t\t\t\t\tprocessed_dataset['history'].append(history)\n","\t\t\t\t\t# processed_dataset['target'].append(reply)\n","\n","\t\t\t\telse:\n","\t\t\t\t\treply = f\"{speaker_1_start} {reply} {speaker_1_end}\"\n","\t\t\t\t\thistory += reply \n","\n","\t\tdataset = pd.DataFrame(data=processed_dataset)\n","\t\treturn dataset\n","\n","train_dataset_csv, valid_dataset_csv = train_test_split(persona_chat_original, test_size=0.1)\n","\n","train_dataset_generator = PersonaChatGenerator(\n","\tinitial_dataset=train_dataset_csv,\n",")\n","\n","valid_dataset_generator = PersonaChatGenerator(\n","\tinitial_dataset=valid_dataset_csv,\n",")\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":12481,"status":"ok","timestamp":1657181820017,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"tdeGQaG6FqK_"},"outputs":[],"source":["class PersonaChatDataset(Dataset):\n","\tdef __init__(self, \n","\t\tinitial_dataset=None,\n","\t\ttokenizer=None\n","\t):\n","\t\tself.initial_dataset = initial_dataset\n","\t\tself.tokenizer = tokenizer\n","\t\n","\tdef __len__(self):\n","\t\treturn len(self.initial_dataset)\n","\t\n","\tdef __getitem__(self, idx):\n","\t\trow = self.initial_dataset.iloc[idx]\n","\t\tpersona = [item.strip() for item in row['persona'].split(\".\") if len(item) > 0 ]\n","\t\trandom.shuffle(persona)\n","\t\tpersona = [torch.tensor(self.tokenizer.encode(item)).flatten() for item in persona]\n","\t\tpersona = torch.cat([*persona])\n","\n","\t\thistory = row['history']\n","\t\thistory = self.tokenizer.encode(history)\n","\t\thistory = torch.tensor(history).flatten()\n","\t\t\n","\t\t# target = row['target']\n","\t\t# target = torch.tensor(self.tokenizer.encode(target)).flatten()\n","\t\t# target = torch.cat([target, torch.tensor([self.tokenizer.eos_token_id], dtype=torch.long)])\n","\n","\t\t# feature = torch.cat([persona, history, torch.tensor([tokenizer.eos_token_id])])\n","\t\tfeature = torch.cat([persona, history])\n","\n","\t\t# target = torch.tensor(target, dtype=torch.long)\n","\t\t# target = target.flatten()\n","\t\treturn {\n","\t\t\t\"feature\": feature,\n","\t\t\t\"target\": feature \n","\t\t}\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n","tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","\n","train_dataset = PersonaChatDataset(\n","\tinitial_dataset=train_dataset_generator.process_dataset(),\n","\ttokenizer=tokenizer\n",")\n","\n","valid_dataset = PersonaChatDataset(\n","\tinitial_dataset=valid_dataset_generator.process_dataset(),\n","\ttokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657181820019,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"utprDGf06OVt"},"outputs":[],"source":["# Args to allow for easy convertion of python script to notebook\n","class Args():\n","    def __init__(self):\n","        self.output_dir = 'output-small-test'\n","        self.model_type = 'gpt2'\n","        self.model_name_or_path = 'microsoft/DialoGPT-small'\n","        self.config_name = 'microsoft/DialoGPT-small'\n","        self.tokenizer_name = 'microsoft/DialoGPT-small'\n","        self.cache_dir = 'cached'\n","        self.block_size = 512\n","        self.do_train = True\n","        self.do_eval = True\n","        self.evaluate_during_training = False\n","        self.per_gpu_train_batch_size = 4\n","        self.per_gpu_eval_batch_size = 4\n","        self.gradient_accumulation_steps = 1\n","        self.learning_rate = 5e-5\n","        self.weight_decay = 0.0\n","        self.adam_epsilon = 1e-8\n","        self.max_grad_norm = 1.0\n","        self.num_train_epochs = 2\n","        self.max_steps = -1\n","        self.warmup_steps = 0\n","        self.logging_steps = 1000\n","        self.save_steps = 3500\n","        self.save_total_limit = None\n","        self.eval_all_checkpoints = False\n","        self.no_cuda = False\n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","        self.should_continue = False\n","        self.seed = 42\n","        self.local_rank = -1\n","        self.fp16 = False\n","        self.fp16_opt_level = 'O1'\n","\n","args = Args()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657181820020,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"naaRHoXgnStq"},"outputs":[],"source":["# Cacheing and storing of data/checkpoints\n","\n","def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n","    return valid_dataset if evaluate else train_dataset\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)"]},{"cell_type":"markdown","metadata":{"id":"pkvMNnrnVHQw"},"source":["## Training and Evaluating\n","\n","There will be quite a lot of code needed for training our model but donâ€™t worry, everything should work as is, the main thing is to give the model the dataset in the right format.\n","\n","Image from [Giphy](https://giphy.com/)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657181820020,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"tXzKlXHeu0Mb"},"outputs":[],"source":["def train(args: Args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size\n","\n","    def collate(examples):\n","        # print(\"EXAMPLES\", examples)\n","        features = [item['feature'] for item in examples]\n","        features = pad_sequence(features, batch_first=True)\n","        \n","        targets = [item['target'] for item in examples]\n","        targets = pad_sequence(targets, batch_first=True)\n","\n","        return {\n","            \"feature\": torch.tensor(features, dtype=torch.long),\n","            \"target\": torch.tensor(targets, dtype=torch.long)\n","        }\n","\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    # Train!\n","    global_step = 1\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            inputs, labels = (batch['feature'], batch['target'])\n","            # print(inputs.shape, labels.shape, inputs, labels)\n","            if inputs.shape[1] > 1024: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","        \n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            \n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if args.evaluate_during_training:\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","\n","                    # _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","# Evaluation of some model\n","\n","def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args.output_dir\n","\n","    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n","    os.makedirs(eval_output_dir, exist_ok=True)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size\n","    # Note that DistributedSampler samples randomly\n","\n","    def collate(examples: List[torch.Tensor]):\n","        features = [item['feature'] for item in examples]\n","        features = pad_sequence(features, batch_first=True)\n","        \n","        targets = [item['target'] for item in examples]\n","        targets = pad_sequence(targets, batch_first=True)\n","\n","        return {\n","            \"feature\": torch.tensor(features, dtype=torch.long),\n","            \"target\": torch.tensor(targets, dtype=torch.long)\n","        }\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    # Eval!\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = (batch['feature'], batch['target'])\n","        inputs = inputs.to(args.device)\n","        labels = labels.to(args.device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","            lm_loss = outputs[0]\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\"perplexity\": perplexity}\n","\n","    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        for key in sorted(result.keys()):\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return result"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657181822667,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"-MGD6bFXV4Z-"},"outputs":[],"source":["# Main runner\n","\n","def main(df_trn, df_val):\n","    args = Args()\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n","    tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","    \n","    model = AutoModelWithLMHead.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=False,\n","        config=config,\n","        cache_dir=args.cache_dir,\n","    )\n","    model.resize_token_embeddings(len(tokenizer))\n","    model.to(args.device)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n","\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    # ...\n","    results = {}\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"UZEHDzR0Vjs7"},"source":["It is time to train our model!"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["863122f18665429cab7e0da1f2d693ec","6eda16c62cc84d83a7527a3f5985e934","77721d2a1ecf453fa6718b20a130e9e5","909c0603d6894251a8fe0ba38d87fd2b","2a894417c73b43f38b3e0c3f120e3213","2f493defe2a24570bab122ed85152645","b4834894c3a3442abaeb707a3d16b140","bc669ca1b3144272ba2974ad6e69f20b","67f0fcae76bb4f0cad446bbc79d318f5","a58334a08d994881b2e35839efceabfc","7db62f83311a4d2db429b66350504b77","6f2ac9fc6de14488927c112b9896697b","42985a873c5649d19898d0a84ac74619","3387723669eb435da7956fb3b016104c","4aa98c116eb24e68a08a6dae89427e24","309c7738760d4f319b2a7294578d12b4","351523c10c804f52ab4869ece8bdf54f","93eb4ed831fd4739ab0b6e09ba6f3a45","868958dc0bb340628190bfacc1517198","cf9e8e25d557474cb38f9470ab757ebf","1a2326dd79a64dc7b56385255a9d68a7","1751d49229ae42fe8fca88a4a9f737c3"]},"id":"__iqR8YFV-Ex","outputId":"3296e725-b2d0-49b7-b970-ac03eba5aee0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/dimweb/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:969: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n","/home/dimweb/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eca22afe32984299a079823e95f49b07","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30e50535489e4fd3a655d2bf95819082","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_22388/1749540694.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"feature\": torch.tensor(features, dtype=torch.long),\n","/tmp/ipykernel_22388/1749540694.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"target\": torch.tensor(targets, dtype=torch.long)\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 1.34 GiB already allocated; 6.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m main(train_dataset, valid_dataset)\n","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 15'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(df_trn, df_val)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=41'>42</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=43'>44</a>\u001b[0m     global_step, tr_loss \u001b[39m=\u001b[39m train(args, train_dataset, model, tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=44'>45</a>\u001b[0m \u001b[39m# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_train:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000014vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m# Create output directory if needed\u001b[39;00m\n","\u001b[1;32m/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=90'>91</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=91'>92</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=92'>93</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=93'>94</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# model outputs are always tuple in transformers (see doc)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blnsigo.mipt.ru/home/dimweb/sandbox/persona_bot/gpt_persona.ipynb#ch0000013vscode-remote?line=96'>97</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1048\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1048\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1049\u001b[0m     input_ids,\n\u001b[1;32m   1050\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1051\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1052\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1053\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1054\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1055\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1056\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1057\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1058\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1059\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1060\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1061\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1062\u001b[0m )\n\u001b[1;32m   1063\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:891\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    881\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    882\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    883\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[1;32m    893\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    894\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    895\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    896\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    898\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    899\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    900\u001b[0m     )\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    392\u001b[0m     hidden_states,\n\u001b[1;32m    393\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    394\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    395\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    396\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    397\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    398\u001b[0m )\n\u001b[1;32m    399\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:332\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    335\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:212\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[1;32m    211\u001b[0m attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mtype(value\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 212\u001b[0m attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_dropout(attn_weights)\n\u001b[1;32m    214\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 1.34 GiB already allocated; 6.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["main(train_dataset, valid_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1657179332948,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"Kzjw63Xeh60a","outputId":"bbefdce6-6088-4822-fc1b-92be33ab88e4"},"outputs":[{"data":{"text/plain":["' i like to smell my own farts. my beer gut is so huge i haven t seen my feet in two years. i am from san fransico. i am always the one who buys the beers. i like to place blame on other people even when i know it is my fault.'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["valid_dataset_csv = valid_dataset_csv.reset_index()\n","valid_dataset_csv['Persona'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"6eDkPEuvbD47"},"source":["## Chatting with  Bot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n","tokenizer.add_tokens(list(SPECIAL_TOKENS.values()), special_tokens=True)\n","model = AutoModelWithLMHead.from_pretrained('output-small')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21932,"status":"ok","timestamp":1657181731404,"user":{"displayName":"dim web","userId":"03939316973290678021"},"user_tz":-180},"id":"nIeqMwZktv7N","outputId":"c56f2656-10cf-4a97-ade3-ddb4a92c06b8"},"outputs":[{"name":"stdout","output_type":"stream","text":[" my favorite food is spaghetti and meatballs. i was raised by two mothers. i am not afraid of what others think. my boyfriend works for nasa. i can be quite forgetful.\n","Hi. What is your name?\n","Bot:  i am mark. nice to meet you \n","What do you like?\n","Bot:  pizza. i am a huge fan of spaghetti and italian meatballs \n","What is your job?\n","Bot:  my job is in the military. i have a degree in biology \n"]}],"source":["# valid_dataset_csv = valid_dataset_csv.reset_index()\n","persona = valid_dataset_csv.iloc[6]['Persona']\n","print(persona)\n","persona = f\"{SPECIAL_TOKENS['<persona>']} {persona} {SPECIAL_TOKENS['</persona>']}\"\n","chat_history_ids = tokenizer.encode(persona, return_tensors='pt')\n","VOCAB_TOKENS = tokenizer.get_added_vocab()\n","\n","user_inputs = [\n","    \"Hi. What is your name?\",\n","    \"What do you like?\",\n","    \"What is your job?\"\n","]\n","\n","last_index = 0\n","\n","for step in range(len(user_inputs)):\n","    # encode the new user input, add the eos_token and return a tensor in Pytorch\n","    # input('>> User:')\n","    print(user_inputs[step])\n","    user_input = f\"{SPECIAL_TOKENS['<speaker_1>']} {user_inputs[step]} {SPECIAL_TOKENS['</speaker_1>']}{SPECIAL_TOKENS['<speaker_2>']}\"\n","    user_input\n","    new_user_input_ids = tokenizer.encode(user_input, return_tensors='pt')\n","    # print(new_user_input_ids)\n","\n","    # append the new user input tokens to the chat history\n","    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","\n","    # generated a response while limiting the total chat history to 1000 tokens, \n","    model_response = model.generate(\n","        bot_input_ids, max_length=250,\n","        pad_token_id=tokenizer.eos_token_id,  \n","        no_repeat_ngram_size=3,       \n","        do_sample=True, \n","        top_k=100,\n","        top_p=0.7,\n","        temperature = 0.8,\n","    )\n","\n","    # print(\"-\"*100)\n","    # print(tokenizer.decode(model_response[0]))\n","    # print(\"-\"*100)\n","    # print(list(model_response[0]), list(model_response[0]).index(VOCAB_TOKENS[\"</speaker_2>\"]))\n","    model_response_list = list(model_response[0])\n","    end_speaker_index = last_index+model_response_list[last_index+1:].index(VOCAB_TOKENS[\"</speaker_2>\"]) +1\n","    last_index = end_speaker_index\n","    model_response = model_response[:, :end_speaker_index+1]\n","    # print(chat_history_ids, torch.tensor([[VOCAB_TOKENS['<speaker_2>']]], dtype=torch.long), model_response)\n","\n","    chat_history_ids = model_response\n","    # print(\"-\"*100)\n","    # print(tokenizer.decode(chat_history_ids[0]))\n","    # print(\"-\"*100)\n","    # chat_history_ids = torch.cat([\n","    #     chat_history_ids, \n","    #     torch.tensor([[VOCAB_TOKENS['</speaker_2>']]], dtype=torch.long), \n","    # ], dim=1)\n","    # print(model_response)\n","    # pretty print last ouput tokens from bot\n","    # print(chat_history_ids)\n","    print(f\"Bot: {tokenizer.decode(chat_history_ids[0][len(bot_input_ids[0])-1:], skip_special_tokens=True)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"gpt_persona.ipynb","provenance":[{"file_id":"1NP986rQFTX9yI07fcvuHaZ5lVs9ya3l0","timestamp":1657101427139},{"file_id":"15wa925dj7jvdvrz8_z3vU7btqAFQLVlG","timestamp":1657100391790}]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"1751d49229ae42fe8fca88a4a9f737c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a2326dd79a64dc7b56385255a9d68a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a894417c73b43f38b3e0c3f120e3213":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f493defe2a24570bab122ed85152645":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309c7738760d4f319b2a7294578d12b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3387723669eb435da7956fb3b016104c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_868958dc0bb340628190bfacc1517198","max":2492,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf9e8e25d557474cb38f9470ab757ebf","value":1259}},"351523c10c804f52ab4869ece8bdf54f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42985a873c5649d19898d0a84ac74619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_351523c10c804f52ab4869ece8bdf54f","placeholder":"â€‹","style":"IPY_MODEL_93eb4ed831fd4739ab0b6e09ba6f3a45","value":"Iteration:  51%"}},"4aa98c116eb24e68a08a6dae89427e24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a2326dd79a64dc7b56385255a9d68a7","placeholder":"â€‹","style":"IPY_MODEL_1751d49229ae42fe8fca88a4a9f737c3","value":" 1259/2492 [05:56&lt;05:28,  3.75it/s]"}},"67f0fcae76bb4f0cad446bbc79d318f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6eda16c62cc84d83a7527a3f5985e934":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f493defe2a24570bab122ed85152645","placeholder":"â€‹","style":"IPY_MODEL_b4834894c3a3442abaeb707a3d16b140","value":"Epoch:   0%"}},"6f2ac9fc6de14488927c112b9896697b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42985a873c5649d19898d0a84ac74619","IPY_MODEL_3387723669eb435da7956fb3b016104c","IPY_MODEL_4aa98c116eb24e68a08a6dae89427e24"],"layout":"IPY_MODEL_309c7738760d4f319b2a7294578d12b4"}},"77721d2a1ecf453fa6718b20a130e9e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc669ca1b3144272ba2974ad6e69f20b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67f0fcae76bb4f0cad446bbc79d318f5","value":0}},"7db62f83311a4d2db429b66350504b77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"863122f18665429cab7e0da1f2d693ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6eda16c62cc84d83a7527a3f5985e934","IPY_MODEL_77721d2a1ecf453fa6718b20a130e9e5","IPY_MODEL_909c0603d6894251a8fe0ba38d87fd2b"],"layout":"IPY_MODEL_2a894417c73b43f38b3e0c3f120e3213"}},"868958dc0bb340628190bfacc1517198":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"909c0603d6894251a8fe0ba38d87fd2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a58334a08d994881b2e35839efceabfc","placeholder":"â€‹","style":"IPY_MODEL_7db62f83311a4d2db429b66350504b77","value":" 0/1 [00:00&lt;?, ?it/s]"}},"93eb4ed831fd4739ab0b6e09ba6f3a45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a58334a08d994881b2e35839efceabfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4834894c3a3442abaeb707a3d16b140":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc669ca1b3144272ba2974ad6e69f20b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9e8e25d557474cb38f9470ab757ebf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
